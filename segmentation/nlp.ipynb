{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_it(x):\n",
    "    return re.sub(\"( [,.;\\)]|[\\(] | ['] | [-] | [:])\", lambda m: m.group(1).strip(), \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trwiki-67/trwiki-67.train.txt\") as fi:\n",
    "    content = fi.read()\n",
    "    wiki_articles = [ { \"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in zip(re.findall(\"== .* ==\", content), re.split(\"== .* == \\n\\n\", content)[1:]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trnews-64/trnews-64.train.raw\") as fi:\n",
    "    news_articles = re.split(\"\\n\\n\", fi.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in open(\"/scratch/users/user/nlp_datasets/mukayese_tokenization/clean/train_news_clean.jsonl\"):\n",
    "    texts.append(json.loads(i)[\"text\"])\n",
    "\n",
    "for i in open(\"/scratch/users/user/nlp_datasets/mukayese_tokenization/clean/abstracts_train_clean.jsonl\"):\n",
    "    texts.append(json.loads(i)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = \" \".join([do_it(i) for i in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [1.3551] t-1\n",
      "  Abbreviation: [0.9034] w-1\n",
      "  Abbreviation: [0.4517] h-j\n",
      "  Abbreviation: [1.2278] r,\n",
      "  Abbreviation: [0.4517] l-8\n",
      "  Abbreviation: [1.8068] a.-v\n",
      "  Collocation: [505.1390] '##number##'+'##number##'\n",
      "  Collocation: [31.7329] 'r'+'iÃá'\n",
      "  Collocation: [149.2189] 'iÃá'+'√∂'\n",
      "  Collocation: [66.8443] 'y'+\"'ƒ±\"\n"
     ]
    }
   ],
   "source": [
    "trainer.train(articles, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = \" \".join([do_it(i) for i in news_articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [0.9062] l.-t\n",
      "  Abbreviation: [1.2316] ≈ü,\n",
      "  Abbreviation: [0.4531] v-w\n",
      "  Removed abbreviation: [-0.0006] r,\n",
      "  Removed abbreviation: [-0.0031] h-j\n",
      "  Abbreviation: [0.9062] s.-f\n",
      "  Abbreviation: [1.3593] √º-4\n",
      "  Abbreviation: [2.4632] r.,\n",
      "  Removed abbreviation: [-0.0031] l-8\n",
      "  Abbreviation: [0.9062] t.-e\n",
      "  Abbreviation: [0.9062] l.-e\n",
      "  Removed abbreviation: [-0.0000] t-1\n",
      "  Abbreviation: [2.1493] z,\n",
      "  Removed abbreviation: [0.1496] w-1\n",
      "  Abbreviation: [0.9177] e,\n",
      "  Abbreviation: [0.4531] √º-3\n",
      "  Abbreviation: [0.9062] y.-a\n",
      "  Abbreviation: [0.6098] m,\n",
      "  Abbreviation: [4.9265] ƒ±.,\n",
      "  Abbreviation: [0.4531] ƒü-9\n",
      "  Abbreviation: [0.9062] v.-p\n",
      "  Abbreviation: [0.4531] o-8\n",
      "  Collocation: [131858.7206] '##number##'+'##number##'\n",
      "  Collocation: [192.7244] 'o'+'√∂.-'\n",
      "  Collocation: [237.4095] 'b'+'y.-'\n",
      "  Collocation: [14.8016] 'w'+'w'\n",
      "  Collocation: [88.7160] 'a'+'≈ü.-'\n",
      "  Collocation: [149.0605] 'h'+'b.-'\n",
      "  Collocation: [125.5559] 'b'+'h.-'\n",
      "  Collocation: [37.3967] 'e'+'g.-'\n",
      "  Collocation: [41.3696] 'a'+'t.-'\n",
      "  Collocation: [72.5172] '√º'+'k.-'\n",
      "  Collocation: [89.0126] '√ß'+'k.-'\n",
      "  Collocation: [116.9794] 's'+'u.-'\n",
      "  Collocation: [88.8220] 'h'+'k.-'\n",
      "  Collocation: [138.7385] 'f'+'g.-'\n",
      "  Collocation: [20.3144] 'a'+'a.-'\n",
      "  Collocation: [97.9582] 'b'+'c.-'\n",
      "  Collocation: [70.2395] 'a'+'√º.-'\n",
      "  Collocation: [75.0716] 'b'+'s.-'\n",
      "  Collocation: [31.1509] 'a'+'√ß.-'\n",
      "  Collocation: [111.6183] 'v'+'v.-'\n",
      "  Collocation: [101.4510] 'iÃá'+'k.-'\n",
      "  Collocation: [25.7661] 'y'+'e.-'\n",
      "  Collocation: [52.1389] 'o'+'s.-'\n",
      "  Collocation: [170.1338] 'j'+'i.-'\n",
      "  Collocation: [19.0827] 't'+'e.-'\n",
      "  Collocation: [20.2031] 'm'+'b.-'\n",
      "  Collocation: [18.5870] 'r'+'a.-'\n",
      "  Collocation: [34.2443] 'e'+'w.-'\n",
      "  Collocation: [27.7354] 'd'+'h.-'\n",
      "  Collocation: [12.6386] 'd'+'b.-'\n",
      "  Collocation: [13.0674] 'd'+'e.-'\n",
      "  Collocation: [15.3296] 's'+'e.-'\n",
      "  Collocation: [44.8611] 'h'+'g.-'\n",
      "  Collocation: [57.8375] 'm'+'m.-'\n",
      "  Collocation: [44.1177] 'g'+'t.-'\n",
      "  Collocation: [87.7968] 'g'+'j.-'\n",
      "  Collocation: [32.4440] 'm'+'w.-'\n",
      "  Collocation: [38.9151] 'a'+'≈ü.-f'\n",
      "  Collocation: [9.6164] 'y'+'b.-'\n",
      "  Collocation: [26.7813] 'h'+'e.-'\n",
      "  Collocation: [10.4851] 'e'+'√ß.-'\n",
      "  Collocation: [28.0522] '≈ü'+'√∂.-'\n",
      "  Collocation: [109.7555] 'iÃá'+'o.-'\n",
      "  Collocation: [34.6101] 'o'+'v.-'\n",
      "  Collocation: [34.5912] 'a'+'≈ü.-t'\n",
      "  Collocation: [54.5203] 'm'+'f.-'\n",
      "  Collocation: [44.8734] 'iÃá'+'e.-'\n",
      "  Collocation: [10.1058] 'm'+'a.-'\n",
      "  Collocation: [30.2673] 'a'+'≈ü.-g'\n",
      "  Collocation: [30.2673] 'a'+'≈ü.-k'\n",
      "  Collocation: [19.0193] 'b'+'≈ü.-'\n",
      "  Collocation: [8.1603] 'r'+'3-b'\n",
      "  Collocation: [25.9434] 'a'+'≈ü.-b'\n",
      "  Collocation: [12.8681] '≈ü'+'g.-'\n",
      "  Collocation: [19.3993] 'h'+'√ß.-'\n",
      "  Collocation: [21.6195] 'a'+'≈ü.-o'\n",
      "  Collocation: [21.6195] 'a'+'≈ü.-m'\n",
      "  Collocation: [26.2799] 'n'+'p.-'\n",
      "  Collocation: [41.7098] 'j'+'h.-'\n",
      "  Collocation: [11.6587] 'h'+'t.-'\n",
      "  Collocation: [17.2956] 'a'+'≈ü.-d'\n",
      "  Collocation: [12.9717] 'a'+'≈ü.-a'\n",
      "  Collocation: [34.5204] 'w'+'g-l'\n",
      "  Collocation: [14.3183] '##number##'+'5-i'\n",
      "  Collocation: [8.6478] 'a'+'≈ü.-√ß'\n",
      "  Collocation: [8.6478] 'a'+'≈ü.-y'\n"
     ]
    }
   ],
   "source": [
    "trainer.train(news_articles, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_articles = \" \".join([do_it(i) for i in wiki_articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collocation: [132271.4600] '##number##'+'##number##'\n",
      "  Collocation: [192.8814] 'o'+'√∂.-'\n",
      "  Collocation: [237.5594] 'b'+'y.-'\n",
      "  Collocation: [14.8796] 'w'+'w'\n",
      "  Collocation: [88.8432] 'a'+'≈ü.-'\n",
      "  Collocation: [149.1851] 'h'+'b.-'\n",
      "  Collocation: [125.6632] 'b'+'h.-'\n",
      "  Collocation: [37.4844] 'e'+'g.-'\n",
      "  Collocation: [41.4545] 'a'+'t.-'\n",
      "  Collocation: [72.6068] '√º'+'k.-'\n",
      "  Collocation: [89.1002] '√ß'+'k.-'\n",
      "  Collocation: [117.0698] 's'+'u.-'\n",
      "  Collocation: [88.9096] 'h'+'k.-'\n",
      "  Collocation: [138.8286] 'f'+'g.-'\n",
      "  Collocation: [20.3774] 'a'+'a.-'\n",
      "  Collocation: [98.0389] 'b'+'c.-'\n",
      "  Collocation: [70.3208] 'a'+'√º.-'\n",
      "  Collocation: [75.1425] 'b'+'s.-'\n",
      "  Collocation: [31.2154] 'a'+'√ß.-'\n",
      "  Collocation: [111.6906] 'v'+'v.-'\n",
      "  Collocation: [101.5184] 'iÃá'+'k.-'\n",
      "  Collocation: [25.8195] 'y'+'e.-'\n",
      "  Collocation: [52.1956] 'o'+'s.-'\n",
      "  Collocation: [170.1929] 'j'+'i.-'\n",
      "  Collocation: [19.1297] 't'+'e.-'\n",
      "  Collocation: [20.2507] 'm'+'b.-'\n",
      "  Collocation: [18.6343] 'r'+'a.-'\n",
      "  Collocation: [34.2993] 'e'+'w.-'\n",
      "  Collocation: [27.7838] 'd'+'h.-'\n",
      "  Collocation: [12.6782] 'd'+'b.-'\n",
      "  Collocation: [13.1074] 'd'+'e.-'\n",
      "  Collocation: [15.3687] 's'+'e.-'\n",
      "  Collocation: [44.9089] 'h'+'g.-'\n",
      "  Collocation: [57.8869] 'm'+'m.-'\n",
      "  Collocation: [44.1616] 'g'+'t.-'\n",
      "  Collocation: [87.8423] 'g'+'j.-'\n",
      "  Collocation: [32.4829] 'm'+'w.-'\n",
      "  Collocation: [38.9561] 'a'+'≈ü.-f'\n",
      "  Collocation: [9.6460] 'y'+'b.-'\n",
      "  Collocation: [26.8187] 'h'+'e.-'\n",
      "  Collocation: [10.5165] 'e'+'√ß.-'\n",
      "  Collocation: [28.0899] '≈ü'+'√∂.-'\n",
      "  Collocation: [109.7964] 'iÃá'+'o.-'\n",
      "  Collocation: [34.6491] 'o'+'v.-'\n",
      "  Collocation: [34.6276] 'a'+'≈ü.-t'\n",
      "  Collocation: [54.5567] 'm'+'f.-'\n",
      "  Collocation: [44.9090] 'iÃá'+'e.-'\n",
      "  Collocation: [10.1309] 'm'+'a.-'\n",
      "  Collocation: [30.2992] 'a'+'≈ü.-g'\n",
      "  Collocation: [30.2992] 'a'+'≈ü.-k'\n",
      "  Collocation: [19.0482] 'b'+'≈ü.-'\n",
      "  Collocation: [8.1818] 'r'+'3-b'\n",
      "  Collocation: [25.9707] 'a'+'≈ü.-b'\n",
      "  Collocation: [12.8916] '≈ü'+'g.-'\n",
      "  Collocation: [19.4209] 'h'+'√ß.-'\n",
      "  Collocation: [21.6423] 'a'+'≈ü.-o'\n",
      "  Collocation: [21.6423] 'a'+'≈ü.-m'\n",
      "  Collocation: [26.3026] 'n'+'p.-'\n",
      "  Collocation: [41.7324] 'j'+'h.-'\n",
      "  Collocation: [11.6753] 'h'+'t.-'\n",
      "  Collocation: [17.3138] 'a'+'≈ü.-d'\n",
      "  Collocation: [12.9854] 'a'+'≈ü.-a'\n",
      "  Collocation: [34.5340] 'w'+'g-l'\n",
      "  Collocation: [14.3274] '##number##'+'5-i'\n",
      "  Collocation: [8.6569] 'a'+'≈ü.-√ß'\n",
      "  Collocation: [8.6569] 'a'+'≈ü.-y'\n"
     ]
    }
   ],
   "source": [
    "trainer.train(wiki_articles, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60794505it [01:33, 647893.68it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "for j,line in tqdm(enumerate(open(\"tagged_tweets.xml\"))):\n",
    "    if line.startswith(\"<tweet>\"):\n",
    "        start = j\n",
    "        sentence = []\n",
    "    elif line.startswith(\"</tweet>\"):\n",
    "        tweets.append(do_it(sentence))\n",
    "    else:\n",
    "        sentence.append(line.split(\"\\t\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets = do_it([do_it(i) for i in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [13.5072] ùìú\n",
      "  Abbreviation: [0.5045] z.,,.,\n",
      "  Abbreviation: [0.9142] e.-3\n",
      "  Abbreviation: [0.4571] w-5\n",
      "  Removed abbreviation: [-0.0009] r.,\n",
      "  Abbreviation: [0.6186] i.,.,.,.,\n",
      "  Abbreviation: [3.3776] ùëò\n",
      "  Abbreviation: [0.6201] m.,\n",
      "  Removed abbreviation: [-0.0003] l.-e\n",
      "  Removed abbreviation: [-0.0000] e,\n",
      "  Removed abbreviation: [-0.0001] o-8\n",
      "  Removed abbreviation: [-0.0000] z,\n",
      "  Abbreviation: [2.4851] c.,\n",
      "  Removed abbreviation: [-0.0000] m,\n",
      "  Abbreviation: [2.4851] a.,\n",
      "  Abbreviation: [2.4839] l.,\n",
      "  Abbreviation: [0.9142] m.-2\n",
      "  Abbreviation: [1.3713] r.,.,\n",
      "  Abbreviation: [0.4571] _-7\n",
      "  Removed abbreviation: [0.1522] √º-3\n",
      "  Removed abbreviation: [-0.1018] l.-t\n",
      "  Abbreviation: [0.3363] 'a-2\n",
      "  Abbreviation: [0.4571] ◊õ-3\n",
      "  Abbreviation: [0.4571] ‚Äî-k\n",
      "  Abbreviation: [1.2414] s.,\n",
      "  Abbreviation: [1.3713] i.,.,\n",
      "  Removed abbreviation: [-0.0000] ≈ü,\n",
      "  Abbreviation: [1.8284] √º-4\n",
      "  Abbreviation: [0.5045] l,.,.,\n",
      "  Abbreviation: [17.3958] h.,\n",
      "  Abbreviation: [0.6201] ƒ±.,\n",
      "  Abbreviation: [0.6726] l.,.,.,\n",
      "  Rare Abbrev: .,.,.,.,,.,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: l.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,,,.\n",
      "  Rare Abbrev: .,.,.,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.\n",
      "  Rare Abbrev: .,.,.,,.,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: i.,.,.,,.,.,.,,.,.,.,.,.\n",
      "  Collocation: [94787.7323] 't'+'c'\n",
      "  Collocation: [15032.7253] '##number##'+'##number##'\n",
      "  Collocation: [187.9531] 'o'+'√∂.-'\n",
      "  Collocation: [230.8251] 'b'+'y.-'\n",
      "  Collocation: [71.2489] 'a'+'≈ü.-'\n",
      "  Collocation: [138.4856] 'h'+'b.-'\n",
      "  Collocation: [124.6193] 'b'+'h.-'\n",
      "  Collocation: [39.7670] 'e'+'g.-'\n",
      "  Collocation: [36.8035] 'a'+'t.-'\n",
      "  Collocation: [72.3849] '√º'+'k.-'\n",
      "  Collocation: [85.8922] '√ß'+'k.-'\n",
      "  Collocation: [94.8543] 's'+'u.-'\n",
      "  Collocation: [73.8735] 'h'+'k.-'\n",
      "  Collocation: [134.0577] 'f'+'g.-'\n",
      "  Collocation: [16.1459] 'a'+'a.-'\n",
      "  Collocation: [95.5142] 'b'+'c.-'\n",
      "  Collocation: [60.4604] 'a'+'√º.-'\n",
      "  Collocation: [71.1225] 'b'+'s.-'\n",
      "  Collocation: [30.4445] 'a'+'√ß.-'\n",
      "  Collocation: [107.0133] 'v'+'v.-'\n",
      "  Collocation: [99.2255] 'iÃá'+'k.-'\n",
      "  Collocation: [22.4384] 'y'+'e.-'\n",
      "  Collocation: [47.9291] 'o'+'s.-'\n",
      "  Collocation: [136.2410] 'j'+'i.-'\n",
      "  Collocation: [17.2849] 't'+'e.-'\n",
      "  Collocation: [18.0099] 'm'+'b.-'\n",
      "  Collocation: [16.8360] 'r'+'a.-'\n",
      "  Collocation: [35.7251] 'e'+'w.-'\n",
      "  Collocation: [29.6580] 'd'+'h.-'\n",
      "  Collocation: [14.2288] 'd'+'b.-'\n",
      "  Collocation: [12.9826] 'd'+'e.-'\n",
      "  Collocation: [11.2731] 's'+'e.-'\n",
      "  Collocation: [40.8196] 'h'+'g.-'\n",
      "  Collocation: [30.1885] 'm'+'m.-'\n",
      "  Collocation: [41.5872] 'g'+'t.-'\n",
      "  Collocation: [86.8552] 'g'+'j.-'\n",
      "  Collocation: [8.1279] 'n'+'√∂.-'\n",
      "  Collocation: [30.6314] 'm'+'w.-'\n",
      "  Collocation: [38.4659] 'a'+'≈ü.-f'\n",
      "  Collocation: [9.0010] 'y'+'b.-'\n",
      "  Collocation: [22.1225] 'h'+'e.-'\n",
      "  Collocation: [11.3354] 'e'+'√ß.-'\n",
      "  Collocation: [29.1958] '≈ü'+'√∂.-'\n",
      "  Collocation: [105.3862] 'iÃá'+'o.-'\n",
      "  Collocation: [33.4254] 'o'+'v.-'\n",
      "  Collocation: [28.1641] 'a'+'≈ü.-t'\n",
      "  Collocation: [52.8202] 'm'+'f.-'\n",
      "  Collocation: [45.2505] 'iÃá'+'e.-'\n",
      "  Collocation: [24.1408] 'a'+'≈ü.-g'\n",
      "  Collocation: [29.9179] 'a'+'≈ü.-k'\n",
      "  Collocation: [16.4174] 'b'+'≈ü.-'\n",
      "  Collocation: [17.1489] 'a'+'≈ü.-b'\n",
      "  Collocation: [13.5813] '≈ü'+'g.-'\n",
      "  Collocation: [17.5742] 'h'+'√ß.-'\n",
      "  Collocation: [16.2144] 'a'+'≈ü.-o'\n",
      "  Collocation: [11.5384] 'a'+'≈ü.-m'\n",
      "  Collocation: [21.5553] 'n'+'p.-'\n",
      "  Collocation: [39.9873] 'j'+'h.-'\n",
      "  Collocation: [161.4797] 'ùìö'+'ùìê'\n",
      "  Collocation: [9.7141] 'h'+'t.-'\n",
      "  Collocation: [8.2887] 'a'+'≈ü.-d'\n",
      "  Collocation: [83.6343] '◊™'+'◊ë'\n",
      "  Collocation: [27.8703] 'w'+'g-l'\n",
      "  Collocation: [25.0070] 'z'+'‚õî-i'\n",
      "  Collocation: [8.6108] '##number##'+'9-g'\n",
      "  Collocation: [8.5480] 'a'+'≈ü.-√ß'\n",
      "  Collocation: [18.3735] 'ÿØ'+'ÿß'\n",
      "  Collocation: [22.5937] 'Ÿá'+'ŸÖ'\n",
      "  Collocation: [19.6263] 'ŸÖ'+'ŸÖ'\n",
      "  Collocation: [29.9622] 'œÇ'+'Œ±'\n",
      "  Collocation: [25.6592] 'ÿØ'+'ÿπ'\n",
      "  Collocation: [11.0678] 'r'+'‚ùû-m'\n",
      "  Collocation: [12.8967] 'ƒ±'+'h-a-m-s-iÃá'\n",
      "  Collocation: [33.1463] '–∞'+'–∫'\n",
      "  Collocation: [21.0030] 'ŸÜ'+'Ÿà'\n",
      "  Collocation: [34.9196] '—Å'+'–≤'\n",
      "  Collocation: [16.8337] 'ÿß'+'ŸÜ'\n",
      "  Collocation: [35.1704] '◊î'+'◊î'\n",
      "  Collocation: [34.6525] '—É'+'–æ'\n"
     ]
    }
   ],
   "source": [
    "trainer.train(raw_tweets, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.finalize_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer,open(\"all_punkt.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zemberek import TurkishSentenceExtractor\n",
    "extractor = TurkishSentenceExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2561955/2561955 [02:11<00:00, 19466.17it/s]\n"
     ]
    }
   ],
   "source": [
    "punkt_tokenized = []\n",
    "for tweet in tqdm(tweets):\n",
    "    punkt_tokenized.append(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2561955/2561955 [02:02<00:00, 20884.74it/s]\n"
     ]
    }
   ],
   "source": [
    "zemberek_tokenized = []\n",
    "for tweet in tqdm(tweets):\n",
    "    zemberek_tokenized.append(extractor.from_paragraph(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_punkt = [i for i in punkt_tokenized if len(i) > 1]\n",
    "ms_zemberek = [i for i in zemberek_tokenized if len(i) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1177457, 1274854)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms_punkt), len(ms_zemberek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisentence_tweets = random.choices(ms_zemberek, k=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlesentence_tweets = random.choices(zemberek_tokenized, k=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = multisentence_tweets[:4000] + singlesentence_tweets[:1000]\n",
    "test = multisentence_tweets[4000:] + singlesentence_tweets[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pandemi s√ºrecinde var olan √∂ƒüretmen ihtiyacƒ± √ºcretli yerine 2019 KPSSde karma atama maƒüdurlarƒ± olan √∂ƒüretmenlerle giderilmeli.',\n",
       " '@user @user @user @user @user #Maliyeden√ñƒürt20Bin #Maliyeden√ñƒürt20Bin']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[4] = ['#AlaattinCakƒ±cƒ± yla ilgili olumsuz yazdƒ±m diye sokak arasƒ± mafyacƒ±lƒ±k oynayan oƒülancƒ±klar hemen tehditlere ba≈üladƒ± üòÇ',\n",
    "            'Sizin √∂z√ºn√ºz bu i≈üte Tehdit, ≈ûantaj, Gasp...',\n",
    " 'ama kuru sƒ±kƒ±sƒ±nƒ±z arkanƒ±zda biri varken sesiniz √ßƒ±kar, yokken kuyruƒüunuz bacaklarƒ±nƒ±zƒ±n arasƒ±nda gezersiniz !!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can you see the writings on the wall some.url'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"\"\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\"\", \"some.url\", \"can you see the writings on the wall https://t.co/3MxhHoTB6t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13886, 121505)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(i) for i in train), sum(sum([len(j.split(\" \")) for j in i]) for i in train  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13821, 121090)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(i) for i in test), sum(sum([len(j.split(\" \")) for j in i]) for i in test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/nlp_datasets/trseg-41/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"clean\", \"train_tweets_clean.jsonl\"), \"w\") as data:\n",
    "    for t in train:\n",
    "        data.write(json.dumps(t, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"clean\", \"test_tweets_clean.jsonl\"), \"w\") as data:\n",
    "    for t in test:\n",
    "        data.write(json.dumps(t, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' asd @user'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"@\\w+\", \"@user\", \" asd @mansuryavas06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"\"\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\"\"\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train[i])):\n",
    "        train[i][j] = re.sub( \"@\\w+\", \"@user\", re.sub(f, \"some.url\", train[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9475174040325887, 2.9539594337861432)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i) for i in ms_punkt]), np.mean([len(i) for i in ms_zemberek]), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.tr import Turkish\n",
    "nlp = Turkish()\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "for line in open(\"/scratch/users/user/nlp_datasets/mukayese_tokenization/clean/train_news_clean.jsonl\"):\n",
    "    news.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_news_clean.jsonl\")]\n",
    "test_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_news_clean.jsonl\")]\n",
    "\n",
    "train_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_abstracts_clean.jsonl\")]\n",
    "test_abstract = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_abstracts_clean.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = [{\"text\" : do_it(i), \"sentences\" : i} for i in train]\n",
    "test_ = [{\"text\" : do_it(i), \"sentences\" : i} for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = train_ + train_news + train_abstracts\n",
    "final_test = test_ + test_news + test_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"clean\", \"train_all_clean.jsonl\"), \"w\") as data:\n",
    "    for i in final_train:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"clean\", \"test_all_clean.jsonl\"), \"w\") as data:\n",
    "    for i in final_test:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.tr import Turkish\n",
    "nlp = Turkish()\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def handle_metrics(values,predictor):\n",
    "    nlp = Turkish()\n",
    "    sentences = []\n",
    "    \n",
    "    try:\n",
    "        for j,i in enumerate(values):\n",
    "            sentences.append(Doc(nlp.vocab, words=i[\"sentences\"], spaces=np.ones(len(i[\"sentences\"]), dtype=bool)))\n",
    "    except:\n",
    "        print(\"During docing : \" , j)\n",
    "    \n",
    "    tokenized = []\n",
    "    try:\n",
    "        for j,p in enumerate(values):\n",
    "            tokens = predictor(p[\"text\"])\n",
    "            tokenized.append(Doc(nlp.vocab, words=tokens, spaces=np.ones(len(tokens), dtype=bool)))\n",
    "    except:\n",
    "        print(\"In the loop: \", j)\n",
    "    examples = [Example(predicted, reference) for predicted, reference in zip(tokenized, sentences)]\n",
    "    scorer = Scorer()\n",
    "    return Scorer.score_tokenization(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\")]\n",
    "test_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_all_clean.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [25.4046] m.s\n",
      "  Abbreviation: [0.8246] t.a.b\n",
      "  Abbreviation: [1.6493] ≈üek\n",
      "  Abbreviation: [11.9551] m.√∂\n",
      "  Abbreviation: [0.7472] dn\n",
      "  Abbreviation: [1.0995] a.-v\n",
      "  Abbreviation: [47.1928] m\n",
      "  Abbreviation: [3.1215] iv\n",
      "  Abbreviation: [1.4944] re\n",
      "  Abbreviation: [1.4944] z.r\n",
      "  Abbreviation: [2.9888] n.a\n",
      "  Abbreviation: [1.6493] lev\n",
      "  Abbreviation: [1.4944] bk\n",
      "  Abbreviation: [0.3720] subsp\n",
      "  Abbreviation: [1.4944] th\n",
      "  Abbreviation: [0.7472] mv\n",
      "  Abbreviation: [2.9888] b.b\n",
      "  Abbreviation: [0.8246] do√ß\n",
      "  Abbreviation: [0.8246] xix\n",
      "  Abbreviation: [2.9888] r.e\n",
      "  Abbreviation: [4.0071] g\n",
      "  Abbreviation: [2.8858] ≈ü\n",
      "  Abbreviation: [3.2985] //t\n",
      "  Abbreviation: [1.4944] o.o\n",
      "  Abbreviation: [11.8648] c\n",
      "  Abbreviation: [3.7360] sp\n",
      "  Abbreviation: [0.5498] abs\n",
      "  Abbreviation: [11.9764] iÃá.√∂\n",
      "  Abbreviation: [1.4944] xi\n",
      "  Abbreviation: [0.5498] env\n",
      "  Abbreviation: [0.5498] ioh\n",
      "  Abbreviation: [1.4944] a.b\n",
      "  Abbreviation: [1.4944] pf\n",
      "  Abbreviation: [4.8296] iÃá.s\n",
      "  Abbreviation: [0.7472] tƒ±\n",
      "  Abbreviation: [1.4944] y.y\n",
      "  Abbreviation: [0.3034] iÃá.s.4\n",
      "  Abbreviation: [0.3034] a.h.iÃá\n",
      "  Abbreviation: [1.4944] hz\n",
      "  Abbreviation: [0.8246] req\n",
      "  Abbreviation: [1.4944] a.y\n",
      "  Abbreviation: [2.2416] vd\n",
      "  Abbreviation: [1.4944] ƒ±r\n",
      "  Abbreviation: [2.7525] j\n",
      "  Abbreviation: [1.4944] √∂.y\n",
      "  Abbreviation: [0.7472] 'z\n",
      "  Abbreviation: [2.6566] yy\n",
      "  Abbreviation: [2.2416] xx\n",
      "  Abbreviation: [0.5498] iÃá.b\n",
      "  Abbreviation: [0.8246] m.a.b\n",
      "  Abbreviation: [0.7472] uz\n",
      "  Abbreviation: [1.2535] st\n",
      "  Abbreviation: [0.7472] mk\n",
      "  Abbreviation: [1.4944] g.y\n",
      "  Abbreviation: [2.9888] s.k\n",
      "  Abbreviation: [0.8246] cad\n",
      "  Abbreviation: [2.9888] t.c\n",
      "  Abbreviation: [0.7472] nr\n",
      "  Abbreviation: [0.4045] 'tur\n",
      "  Abbreviation: [0.7472] l√†\n",
      "  Abbreviation: [0.7472] aw\n",
      "  Abbreviation: [1.4944] m.a\n",
      "  Abbreviation: [0.7472] md\n",
      "  Abbreviation: [2.9888] a.≈ü\n",
      "  Abbreviation: [12.3695] res\n",
      "  Abbreviation: [0.7472] up\n",
      "  Rare Abbrev: t√πrkiye.\n",
      "  Rare Abbrev: kastetmemi≈ütim.\n",
      "  Rare Abbrev: raporunu.\n",
      "  Rare Abbrev: adamlardan.\n",
      "  Rare Abbrev: √ßoƒüalƒ±yor.\n",
      "  Rare Abbrev: yaratƒ±ldƒ±.\n",
      "  Rare Abbrev: unutuldu.\n",
      "  Rare Abbrev: aƒüliyor.\n",
      "  Rare Abbrev: √ßoƒüalƒ±r.\n",
      "  Rare Abbrev: √ßalƒ±≈ümalƒ±yƒ±z.\n",
      "  Rare Abbrev: getirdi.\n",
      "  Rare Abbrev: oynasƒ±n.\n",
      "  Rare Abbrev: alacaksƒ±n.\n",
      "  Rare Abbrev: bitiremediniz.\n",
      "  Rare Abbrev: ara≈ütƒ±rƒ±yormu≈ü.\n",
      "  Rare Abbrev: alamaz.\n",
      "  Rare Abbrev: konu≈üuyorlar.\n",
      "  Rare Abbrev: keyifsizim.\n",
      "  Rare Abbrev: azalƒ±yor.\n",
      "  Rare Abbrev: baksƒ±n.\n",
      "  Rare Abbrev: g√∂remiyorum.\n",
      "  Rare Abbrev: kazanƒ±r.\n",
      "  Rare Abbrev: bilirsin.\n",
      "  Rare Abbrev: deƒüiÃáldiÃár.\n",
      "  Rare Abbrev: l√ºtfusun.\n",
      "  Rare Abbrev: bƒ±rakarak.\n",
      "  Rare Abbrev: √∂ƒüret.\n",
      "  Rare Abbrev: √ºz√ºld√ºm.\n",
      "  Rare Abbrev: g√∂z√ºk√ºyor.\n",
      "  Rare Abbrev: siyasetci.\n",
      "  Rare Abbrev: bulundular.\n",
      "  Rare Abbrev: geliniyor.\n",
      "  Rare Abbrev: akpartiyi.\n",
      "  Rare Abbrev: √ßƒ±karmadƒ±.\n",
      "  Rare Abbrev: ispatladiniz.\n",
      "  Rare Abbrev: ele≈ütirmedim.\n",
      "  Rare Abbrev: boƒüulursunuz.\n",
      "  Rare Abbrev: uydurdular.\n",
      "  Rare Abbrev: yoruldum.\n",
      "  Rare Abbrev: yaparƒ±z.\n",
      "  Rare Abbrev: yeti≈ütiriniz.\n",
      "  Rare Abbrev: kar≈üƒ±la≈üabilirsin.\n",
      "  Rare Abbrev: √∂rn.\n",
      "  Rare Abbrev: gitmeyin.\n",
      "  Rare Abbrev: kazanacaklar.\n",
      "  Rare Abbrev: u√ßuyor.\n",
      "  Rare Abbrev: y√ºzdendir.\n",
      "  Rare Abbrev: artmƒ±≈ütƒ±.\n",
      "  Rare Abbrev: bekledik.\n",
      "  Rare Abbrev: yiyor.\n",
      "  Rare Abbrev: zorla≈ütƒ±rmakta.\n",
      "  Rare Abbrev: veremiyoruz.\n",
      "  Rare Abbrev: bulunsun.\n",
      "  Rare Abbrev: gelmezdi.\n",
      "  Rare Abbrev: g√∂nderilecek.\n",
      "  Rare Abbrev: xviii.\n",
      "  Rare Abbrev: boiss.\n",
      "  Rare Abbrev: bal.\n",
      "  Rare Abbrev: hub.-mor.\n",
      "  Rare Abbrev: sint.\n",
      "  Rare Abbrev: bieb.\n",
      "  Rare Abbrev: xiv.\n",
      "  Sent Starter: [1169.6559] 'bu'\n",
      "  Sent Starter: [330.2720] 'ancak'\n",
      "  Sent Starter: [54.0367] 'mansur'\n",
      "  Sent Starter: [107.3979] 'ama'\n",
      "  Sent Starter: [286.1316] 'ayrƒ±ca'\n",
      "  Sent Starter: [51.4590] 'her'\n",
      "  Sent Starter: [59.5287] 'o'\n",
      "  Sent Starter: [37.3793] 'pandemi'\n",
      "  Sent Starter: [65.5599] 'ankara'\n",
      "  Sent Starter: [103.5563] 'biz'\n",
      "  Sent Starter: [51.6848] 't√ºrkiye'\n",
      "  Sent Starter: [69.0097] 'ben'\n",
      "  Sent Starter: [123.7092] 'bunun'\n",
      "  Sent Starter: [112.8084] '≈üimdi'\n",
      "  Sent Starter: [115.5124] '√ß√ºnk√º'\n",
      "  Sent Starter: [106.2883] 'buna'\n",
      "  Sent Starter: [53.4471] '√∂zellikle'\n",
      "  Sent Starter: [44.1705] 'allah'\n",
      "  Sent Starter: [80.6937] 'dolayƒ±sƒ±yla'\n",
      "  Sent Starter: [91.5937] 'sonu√ß'\n",
      "  Sent Starter: [56.0503] 'bunlar'\n",
      "  Sent Starter: [46.5750] 'burada'\n",
      "  Sent Starter: [37.0440] 'yani'\n",
      "  Sent Starter: [33.4633] 'l√ºtfen'\n",
      "  Sent Starter: [75.7375] 'iÃálk'\n",
      "  Sent Starter: [73.0561] 'eƒüer'\n",
      "  Sent Starter: [105.1134] 'b√∂ylece'\n",
      "  Sent Starter: [44.9623] 'fakat'\n",
      "  Sent Starter: [43.7066] 'bunlarƒ±n'\n",
      "  Sent Starter: [56.1890] 'iÃá≈üte'\n",
      "  Sent Starter: [36.4867] 'onlar'\n",
      "  Sent Starter: [46.8557] 'umarƒ±m'\n",
      "  Sent Starter: [70.7500] 'iÃáki'\n",
      "  Sent Starter: [72.9681] 'iÃákinci'\n",
      "  Sent Starter: [99.2205] 'nitekim'\n",
      "  Sent Starter: [51.4395] 'iÃáyi'\n",
      "  Sent Starter: [52.8602] 'ama√ß'\n",
      "  Sent Starter: [68.0923] 'vekhssere'\n",
      "  Sent Starter: [40.6762] '√∂rneƒüin'\n",
      "  Sent Starter: [42.8539] 'bununla'\n",
      "  Sent Starter: [33.8643] 'hele'\n",
      "  Sent Starter: [44.7956] 'oysa'\n",
      "  Sent Starter: [46.6767] 'bunlardan'\n",
      "  Sent Starter: [37.3185] 'rhodiapolis'\n",
      "  Sent Starter: [32.0912] 'zira'\n",
      "  Sent Starter: [30.0006] '√∂n√ºm√ºzdeki'\n",
      "  Sent Starter: [46.2023] 'bulgular'\n",
      "  Sent Starter: [35.8168] '√ºstelik'\n",
      "  Sent Starter: [60.6305] 'platon'\n",
      "  Sent Starter: [30.3646] 'binanƒ±n'\n",
      "  Sent Starter: [44.5672] 'pekg√ºzel'\n",
      "  Sent Starter: [30.2032] 'iÃánsanlar'\n",
      "  Sent Starter: [37.9459] 'tarƒ±mda'\n",
      "  Sent Starter: [38.5815] 'ailelerimiz'\n",
      "  Collocation: [905.6203] '##number##'+'yy.'\n",
      "  Collocation: [99.8108] '##number##'+'y√ºzyƒ±lƒ±n'\n",
      "  Collocation: [96.9717] '##number##'+'y√ºzyƒ±lda'\n",
      "  Collocation: [70.2145] '##number##'+'y√ºzyƒ±la'\n",
      "  Collocation: [28.6968] 's'+'##number##'\n",
      "  Collocation: [55.6583] '##number##'+'dakikada'\n",
      "  Collocation: [41.7253] '##number##'+'y√ºzyƒ±l'\n",
      "  Collocation: [46.1923] '##number##'+'y√ºzyƒ±ldan'\n",
      "  Collocation: [84.1135] 'a'+'≈ü.'\n",
      "  Collocation: [73.8136] 'iÃá'+'s'\n",
      "  Collocation: [44.7702] 'iÃá'+'melih'\n",
      "  Collocation: [14.2586] '##number##'+'sƒ±nƒ±f'\n",
      "  Collocation: [22.3514] '√∂'+'##number##'\n",
      "  Collocation: [19.6226] '##number##'+'y√ºzyƒ±llarda'\n",
      "  Collocation: [10.1821] '##number##'+'g√ºn√ºnde'\n",
      "  Collocation: [54.7198] 'b'+'iÃáplik√ßioƒülu'\n",
      "  Collocation: [17.1939] '##number##'+'satƒ±rda'\n",
      "  Collocation: [45.6196] 'i'+'al√¢eddin'\n",
      "  Collocation: [19.3874] '##number##'+'y√ºzyƒ±llar'\n",
      "  Collocation: [48.8352] 'p'+'w'\n",
      "  Collocation: [63.8468] 'w'+'ball'\n",
      "  Collocation: [15.8033] 'i'+'melih'\n",
      "  Collocation: [12.1173] '##number##'+'ayet'\n",
      "  Collocation: [16.7517] 's'+'s'\n",
      "  Collocation: [15.8984] '##number##'+'maddesine'\n",
      "  Collocation: [41.2351] 'f'+'t√∂'\n",
      "  Collocation: [15.8984] '##number##'+'maddesinde'\n",
      "  Collocation: [9.2821] '##number##'+'http'\n",
      "  Collocation: [15.8984] '##number##'+'maddesini'\n",
      "  Collocation: [30.7453] 'n'+'√ßevik'\n",
      "  Collocation: [32.7284] 't'+'kodakarensis'\n",
      "  Collocation: [34.0202] 'r'+'heberdey'\n",
      "  Collocation: [15.8984] '##number##'+'satƒ±rlar'\n",
      "  Collocation: [28.6094] 'iÃá'+'√∂'\n"
     ]
    }
   ],
   "source": [
    "text = do_it([i[\"text\"] for i in train_all])\n",
    "trainer.train(text, verbose=True)\n",
    "trainer.finalize_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer,open(\"mukayese_punkt_clean.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = train_ + train_news + train_abstracts\n",
    "final_test = test_ + test_news + test_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9743769743769745,\n",
       " 'token_p': 0.849907108634008,\n",
       " 'token_r': 0.8397256303738769,\n",
       " 'token_f': 0.8447856934590339}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, pickle.load(open(\"/scratch/users/user/nltk_data/tokenizers/punkt/turkish.pickle\", \"rb\")).tokenize) # clean punkt train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.973001534861485,\n",
       " 'token_p': 0.8409447275578205,\n",
       " 'token_r': 0.8290020287894889,\n",
       " 'token_f': 0.8349306738019946}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, pickle.load(open(\"all_punkt.pkl\", \"rb\")).tokenize) # clean punkt train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9721615299591291,\n",
       " 'token_p': 0.8856099634872205,\n",
       " 'token_r': 0.8552796831224037,\n",
       " 'token_f': 0.8701806118687799}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, tokenizer.tokenize) # clean punkt train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9734536283507969,\n",
       " 'token_p': 0.8886581226888203,\n",
       " 'token_r': 0.8641871921182266,\n",
       " 'token_f': 0.8762518418620914}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, tokenizer.tokenize) # clean punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9939250259297674,\n",
       " 'token_p': 0.9534609720176731,\n",
       " 'token_r': 0.9498239436619719,\n",
       " 'token_f': 0.9516389828017052}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_news, tokenizer.tokenize) # clean punkt train news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9950161243037233,\n",
       " 'token_p': 0.9539089848308051,\n",
       " 'token_r': 0.9586631486367634,\n",
       " 'token_f': 0.9562801579178242}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_news, tokenizer.tokenize) # clean punkt test news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9971996639596751,\n",
       " 'token_p': 0.8754537838592572,\n",
       " 'token_r': 0.9198943661971831,\n",
       " 'token_f': 0.897124052081843}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_abstracts, tokenizer.tokenize) # clean punkt train abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.996057404194922,\n",
       " 'token_p': 0.9110901665095822,\n",
       " 'token_r': 0.9452411994784876,\n",
       " 'token_f': 0.9278515437529995}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_abstract, tokenizer.tokenize) # clean punkt test abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9591396129857668,\n",
       " 'token_p': 0.8707075362986864,\n",
       " 'token_r': 0.8162177732968457,\n",
       " 'token_f': 0.8425826116046538}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_, tokenizer.tokenize) # clean punkt train tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9619732785200412,\n",
       " 'token_p': 0.8661843107387662,\n",
       " 'token_r': 0.8228782287822878,\n",
       " 'token_f': 0.8439761047827539}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_, tokenizer.tokenize) # clean punkt test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer = Sentencizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2ab86f7b6f80>"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = Turkish()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bu bir c√ºmledir.\n",
      "bu da bir c√ºmledir\n"
     ]
    }
   ],
   "source": [
    "for i in nlp(\"bu bir c√ºmledir. bu da bir c√ºmledir\").sents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_wrapper(x):\n",
    "    return [i.text for i in nlp(x).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bu bir c√ºmledir.', 'bu da bir c√ºmledir']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_wrapper(\"bu bir c√ºmledir. bu da bir c√ºmledir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9478418223428603,\n",
       " 'token_p': 0.7480904446355405,\n",
       " 'token_r': 0.7143754226644768,\n",
       " 'token_f': 0.7308443082701193}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, dummy_wrapper) # clean spacy train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9479053022037967,\n",
       " 'token_p': 0.7649712241406128,\n",
       " 'token_r': 0.7267980295566503,\n",
       " 'token_f': 0.7453962159294718}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, dummy_wrapper) # clean spacy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9485213581599123,\n",
       " 'token_p': 0.5050595238095238,\n",
       " 'token_r': 0.4979460093896714,\n",
       " 'token_f': 0.5014775413711584}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_news, dummy_wrapper) # clean spacy train news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9633995037220844,\n",
       " 'token_p': 0.5951526032315978,\n",
       " 'token_r': 0.58311345646438,\n",
       " 'token_f': 0.5890715237672146}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_news, dummy_wrapper) # clean spacy test news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.994535519125683,\n",
       " 'token_p': 0.7758152173913043,\n",
       " 'token_r': 0.8377347417840375,\n",
       " 'token_f': 0.80558690744921}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_abstracts, dummy_wrapper) # clean spacy train abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.990764331210191,\n",
       " 'token_p': 0.8239192174187441,\n",
       " 'token_r': 0.8510430247718384,\n",
       " 'token_f': 0.8372615039281706}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_abstract, dummy_wrapper) # clean spacy test abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9333389198474882,\n",
       " 'token_p': 0.8042265692513159,\n",
       " 'token_r': 0.737217341206971,\n",
       " 'token_f': 0.7692654518128875}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_, dummy_wrapper) # clean spacy train tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9324866310160428,\n",
       " 'token_p': 0.7947714464621165,\n",
       " 'token_r': 0.7346791114969973,\n",
       " 'token_f': 0.7635447606872955}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_, dummy_wrapper) # clean spacy test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caustic are the ties that bind  '"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^\\w\\s]', '', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(sentences):\n",
    "    new = []\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        if np.random.uniform() > .5:\n",
    "            s = re.sub(r'[^\\w\\s@]', '', s)\n",
    "        if np.random.uniform() > .50:\n",
    "            u = np.random.uniform()\n",
    "            if u > .5:\n",
    "                s = s.lower()\n",
    "            else:\n",
    "                s = s.upper()\n",
    "        new.append(s)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_abstracts_clean.jsonl\")]\n",
    "test_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_abstracts_clean.jsonl\")]\n",
    "\n",
    "train_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_news_clean.jsonl\")]\n",
    "test_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_news_clean.jsonl\")]\n",
    "\n",
    "train_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_tweets_clean.jsonl\")]\n",
    "test_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_tweets_clean.jsonl\")]\n",
    "\n",
    "\n",
    "train_all = train_abstracts + train_news + train_tweets\n",
    "test_all = test_abstracts + test_news + test_tweets\n",
    "\n",
    "\n",
    "\n",
    "train_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_abstracts_clean.jsonl\")]\n",
    "test_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_abstracts_clean.jsonl\")]\n",
    "\n",
    "train_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_news_clean.jsonl\")]\n",
    "test_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_news_clean.jsonl\")]\n",
    "\n",
    "train_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_tweets_clean.jsonl\")]\n",
    "test_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_tweets_clean.jsonl\")]\n",
    "\n",
    "\n",
    "train_all = train_abstracts + train_news + train_tweets\n",
    "test_all = test_abstracts + test_news + test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    return [re.sub(f, \"some.url\", re.sub(\"@\\w+\", \"@user\", i)) for i in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@mansuryavas06 Ba≈ükanƒ±m Rte talimatƒ± vermi≈ütir \" tm itirazlarƒ± reddedin \" diye.',\n",
       " \"YSK akp'nin ma≈üasƒ± durumundadƒ±r ≈üu an, AHƒ∞M'e gider bu hƒ±rsƒ±zlƒ±k\"]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets_ = [preprocess_tweets(i) for i in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_tweets = [{\"text\" : do_it(i), \"sentences\" : i} for i in test_tweets_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Ba≈ükanƒ±m Rte talimatƒ± vermi≈ütir \" tm itirazlarƒ± reddedin \" diye. YSK akp\\'nin ma≈üasƒ± durumundadƒ±r ≈üu an, AHƒ∞M\\'e gider bu hƒ±rsƒ±zlƒ±k',\n",
       " 'sentences': ['@user Ba≈ükanƒ±m Rte talimatƒ± vermi≈ütir \" tm itirazlarƒ± reddedin \" diye.',\n",
       "  \"YSK akp'nin ma≈üasƒ± durumundadƒ±r ≈üu an, AHƒ∞M'e gider bu hƒ±rsƒ±zlƒ±k\"]}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Ba≈ükanƒ±m Rte talimatƒ± vermi≈ütir \" tm itirazlarƒ± reddedin \" diye. YSK akp\\'nin ma≈üasƒ± durumundadƒ±r ≈üu an, AHƒ∞M\\'e gider bu hƒ±rsƒ±zlƒ±k',\n",
       " 'sentences': ['@user Ba≈ükanƒ±m Rte talimatƒ± vermi≈ütir \" tm itirazlarƒ± reddedin \" diye.',\n",
       "  \"YSK akp'nin ma≈üasƒ± durumundadƒ±r ≈üu an, AHƒ∞M'e gider bu hƒ±rsƒ±zlƒ±k\"]}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets_ = []\n",
    "for i in train_tweets:\n",
    "    train_tweets_.append({\"text\" : do_it(i), \"sentences\" : i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Doƒüum G√ºn√ºn√ºz Kutlu Olsun Ba≈ükanƒ±m. Iyi ki Varsiniz. Allah yolunuzu a√ßƒ±k etsin. Ankara icin yaptiklariniz inanƒ±lmaz üôè üèª üíê #BirlikteBasaracagiz Saygilarimla... üíê some.url',\n",
       " 'sentences': ['@user Doƒüum G√ºn√ºn√ºz Kutlu Olsun Ba≈ükanƒ±m.',\n",
       "  'Iyi ki Varsiniz.',\n",
       "  'Allah yolunuzu a√ßƒ±k etsin.',\n",
       "  'Ankara icin yaptiklariniz inanƒ±lmaz üôè üèª üíê #BirlikteBasaracagiz Saygilarimla...',\n",
       "  'üíê some.url']}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_tweets_clean_v2.jsonl\", \"w\") as data:\n",
    "     for line in train_tweets_:\n",
    "            data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_tweets_clean_v2.jsonl\", \"w\") as data:\n",
    "     for line in final_test_tweets:\n",
    "            data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_abstracts + train_news + train_tweets_\n",
    "test_all = test_abstracts + test_news + final_test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\", \"w\") as data:\n",
    "    for line in train_all:\n",
    "        data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_all_clean.jsonl\", \"w\") as data:\n",
    "    for line in test_all:\n",
    "        data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [25.4046] m.s\n",
      "  Abbreviation: [0.8246] t.a.b\n",
      "  Abbreviation: [1.6493] ≈üek\n",
      "  Abbreviation: [11.9551] m.√∂\n",
      "  Abbreviation: [0.7472] dn\n",
      "  Abbreviation: [1.0995] a.-v\n",
      "  Abbreviation: [47.1928] m\n",
      "  Abbreviation: [3.1215] iv\n",
      "  Abbreviation: [1.4944] re\n",
      "  Abbreviation: [1.4944] z.r\n",
      "  Abbreviation: [2.9888] n.a\n",
      "  Abbreviation: [1.6493] lev\n",
      "  Abbreviation: [1.4944] bk\n",
      "  Abbreviation: [0.3720] subsp\n",
      "  Abbreviation: [1.4944] th\n",
      "  Abbreviation: [0.7472] mv\n",
      "  Abbreviation: [2.9888] b.b\n",
      "  Abbreviation: [0.8246] do√ß\n",
      "  Abbreviation: [0.8246] xix\n",
      "  Abbreviation: [2.9888] r.e\n",
      "  Abbreviation: [4.0071] g\n",
      "  Abbreviation: [2.8858] ≈ü\n",
      "  Abbreviation: [3.2985] //t\n",
      "  Abbreviation: [1.4944] o.o\n",
      "  Abbreviation: [11.8648] c\n",
      "  Abbreviation: [3.7360] sp\n",
      "  Abbreviation: [0.5498] abs\n",
      "  Abbreviation: [11.9764] iÃá.√∂\n",
      "  Abbreviation: [1.4944] xi\n",
      "  Abbreviation: [0.5498] env\n",
      "  Abbreviation: [0.5498] ioh\n",
      "  Abbreviation: [1.4944] a.b\n",
      "  Abbreviation: [1.4944] pf\n",
      "  Abbreviation: [4.8296] iÃá.s\n",
      "  Abbreviation: [0.7472] tƒ±\n",
      "  Abbreviation: [1.4944] y.y\n",
      "  Abbreviation: [0.3034] iÃá.s.4\n",
      "  Abbreviation: [0.3034] a.h.iÃá\n",
      "  Abbreviation: [1.4944] hz\n",
      "  Abbreviation: [0.8246] req\n",
      "  Abbreviation: [1.4944] a.y\n",
      "  Abbreviation: [2.2416] vd\n",
      "  Abbreviation: [1.4944] ƒ±r\n",
      "  Abbreviation: [2.7525] j\n",
      "  Abbreviation: [1.4944] √∂.y\n",
      "  Abbreviation: [0.7472] 'z\n",
      "  Abbreviation: [2.6566] yy\n",
      "  Abbreviation: [2.2416] xx\n",
      "  Abbreviation: [0.5498] iÃá.b\n",
      "  Abbreviation: [0.8246] m.a.b\n",
      "  Abbreviation: [0.7472] uz\n",
      "  Abbreviation: [1.2535] st\n",
      "  Abbreviation: [0.7472] mk\n",
      "  Abbreviation: [1.4944] g.y\n",
      "  Abbreviation: [2.9888] s.k\n",
      "  Abbreviation: [0.8246] cad\n",
      "  Abbreviation: [2.9888] t.c\n",
      "  Abbreviation: [0.7472] nr\n",
      "  Abbreviation: [0.4045] 'tur\n",
      "  Abbreviation: [0.7472] l√†\n",
      "  Abbreviation: [0.7472] aw\n",
      "  Abbreviation: [1.4944] m.a\n",
      "  Abbreviation: [0.7472] md\n",
      "  Abbreviation: [2.9888] a.≈ü\n",
      "  Abbreviation: [12.3695] res\n",
      "  Abbreviation: [0.7472] up\n",
      "  Rare Abbrev: xviii.\n",
      "  Rare Abbrev: boiss.\n",
      "  Rare Abbrev: bal.\n",
      "  Rare Abbrev: hub.-mor.\n",
      "  Rare Abbrev: sint.\n",
      "  Rare Abbrev: bieb.\n",
      "  Rare Abbrev: xiv.\n",
      "  Rare Abbrev: t√πrkiye.\n",
      "  Rare Abbrev: kastetmemi≈ütim.\n",
      "  Rare Abbrev: raporunu.\n",
      "  Rare Abbrev: adamlardan.\n",
      "  Rare Abbrev: √ßoƒüalƒ±yor.\n",
      "  Rare Abbrev: yaratƒ±ldƒ±.\n",
      "  Rare Abbrev: unutuldu.\n",
      "  Rare Abbrev: aƒüliyor.\n",
      "  Rare Abbrev: √ßoƒüalƒ±r.\n",
      "  Rare Abbrev: √ßalƒ±≈ümalƒ±yƒ±z.\n",
      "  Rare Abbrev: getirdi.\n",
      "  Rare Abbrev: oynasƒ±n.\n",
      "  Rare Abbrev: alacaksƒ±n.\n",
      "  Rare Abbrev: bitiremediniz.\n",
      "  Rare Abbrev: ara≈ütƒ±rƒ±yormu≈ü.\n",
      "  Rare Abbrev: alamaz.\n",
      "  Rare Abbrev: konu≈üuyorlar.\n",
      "  Rare Abbrev: keyifsizim.\n",
      "  Rare Abbrev: azalƒ±yor.\n",
      "  Rare Abbrev: baksƒ±n.\n",
      "  Rare Abbrev: g√∂remiyorum.\n",
      "  Rare Abbrev: kazanƒ±r.\n",
      "  Rare Abbrev: bilirsin.\n",
      "  Rare Abbrev: deƒüiÃáldiÃár.\n",
      "  Rare Abbrev: l√ºtfusun.\n",
      "  Rare Abbrev: bƒ±rakarak.\n",
      "  Rare Abbrev: √∂ƒüret.\n",
      "  Rare Abbrev: √ºz√ºld√ºm.\n",
      "  Rare Abbrev: g√∂z√ºk√ºyor.\n",
      "  Rare Abbrev: siyasetci.\n",
      "  Rare Abbrev: bulundular.\n",
      "  Rare Abbrev: geliniyor.\n",
      "  Rare Abbrev: akpartiyi.\n",
      "  Rare Abbrev: √ßƒ±karmadƒ±.\n",
      "  Rare Abbrev: ispatladiniz.\n",
      "  Rare Abbrev: ele≈ütirmedim.\n",
      "  Rare Abbrev: boƒüulursunuz.\n",
      "  Rare Abbrev: uydurdular.\n",
      "  Rare Abbrev: yoruldum.\n",
      "  Rare Abbrev: yaparƒ±z.\n",
      "  Rare Abbrev: yeti≈ütiriniz.\n",
      "  Rare Abbrev: kar≈üƒ±la≈üabilirsin.\n",
      "  Rare Abbrev: √∂rn.\n",
      "  Rare Abbrev: gitmeyin.\n",
      "  Rare Abbrev: kazanacaklar.\n",
      "  Rare Abbrev: u√ßuyor.\n",
      "  Rare Abbrev: y√ºzdendir.\n",
      "  Rare Abbrev: artmƒ±≈ütƒ±.\n",
      "  Rare Abbrev: bekledik.\n",
      "  Rare Abbrev: yiyor.\n",
      "  Rare Abbrev: zorla≈ütƒ±rmakta.\n",
      "  Rare Abbrev: veremiyoruz.\n",
      "  Rare Abbrev: bulunsun.\n",
      "  Rare Abbrev: gelmezdi.\n",
      "  Rare Abbrev: g√∂nderilecek.\n",
      "  Sent Starter: [1169.6559] 'bu'\n",
      "  Sent Starter: [330.2720] 'ancak'\n",
      "  Sent Starter: [54.0367] 'mansur'\n",
      "  Sent Starter: [107.3979] 'ama'\n",
      "  Sent Starter: [286.1316] 'ayrƒ±ca'\n",
      "  Sent Starter: [51.4590] 'her'\n",
      "  Sent Starter: [59.5287] 'o'\n",
      "  Sent Starter: [37.3793] 'pandemi'\n",
      "  Sent Starter: [65.5599] 'ankara'\n",
      "  Sent Starter: [103.5563] 'biz'\n",
      "  Sent Starter: [51.6848] 't√ºrkiye'\n",
      "  Sent Starter: [123.7092] 'bunun'\n",
      "  Sent Starter: [69.0097] 'ben'\n",
      "  Sent Starter: [112.8084] '≈üimdi'\n",
      "  Sent Starter: [115.5124] '√ß√ºnk√º'\n",
      "  Sent Starter: [106.2883] 'buna'\n",
      "  Sent Starter: [53.4471] '√∂zellikle'\n",
      "  Sent Starter: [80.6937] 'dolayƒ±sƒ±yla'\n",
      "  Sent Starter: [44.1705] 'allah'\n",
      "  Sent Starter: [91.5937] 'sonu√ß'\n",
      "  Sent Starter: [56.0503] 'bunlar'\n",
      "  Sent Starter: [46.5750] 'burada'\n",
      "  Sent Starter: [37.0440] 'yani'\n",
      "  Sent Starter: [105.1134] 'b√∂ylece'\n",
      "  Sent Starter: [75.7375] 'iÃálk'\n",
      "  Sent Starter: [73.0561] 'eƒüer'\n",
      "  Sent Starter: [33.4633] 'l√ºtfen'\n",
      "  Sent Starter: [43.7066] 'bunlarƒ±n'\n",
      "  Sent Starter: [44.9623] 'fakat'\n",
      "  Sent Starter: [56.1890] 'iÃá≈üte'\n",
      "  Sent Starter: [72.9681] 'iÃákinci'\n",
      "  Sent Starter: [70.7500] 'iÃáki'\n",
      "  Sent Starter: [36.4867] 'onlar'\n",
      "  Sent Starter: [46.8557] 'umarƒ±m'\n",
      "  Sent Starter: [99.2205] 'nitekim'\n",
      "  Sent Starter: [52.8602] 'ama√ß'\n",
      "  Sent Starter: [68.0923] 'vekhssere'\n",
      "  Sent Starter: [51.4395] 'iÃáyi'\n",
      "  Sent Starter: [40.6762] '√∂rneƒüin'\n",
      "  Sent Starter: [42.8539] 'bununla'\n",
      "  Sent Starter: [46.6767] 'bunlardan'\n",
      "  Sent Starter: [44.7956] 'oysa'\n",
      "  Sent Starter: [37.3185] 'rhodiapolis'\n",
      "  Sent Starter: [33.8643] 'hele'\n",
      "  Sent Starter: [46.2023] 'bulgular'\n",
      "  Sent Starter: [32.0912] 'zira'\n",
      "  Sent Starter: [30.0006] '√∂n√ºm√ºzdeki'\n",
      "  Sent Starter: [35.8168] '√ºstelik'\n",
      "  Sent Starter: [30.3646] 'binanƒ±n'\n",
      "  Sent Starter: [60.6305] 'platon'\n",
      "  Sent Starter: [44.5672] 'pekg√ºzel'\n",
      "  Sent Starter: [30.2032] 'iÃánsanlar'\n",
      "  Sent Starter: [37.9459] 'tarƒ±mda'\n",
      "  Sent Starter: [38.5815] 'ailelerimiz'\n",
      "  Collocation: [905.6203] '##number##'+'yy.'\n",
      "  Collocation: [99.8108] '##number##'+'y√ºzyƒ±lƒ±n'\n",
      "  Collocation: [96.9717] '##number##'+'y√ºzyƒ±lda'\n",
      "  Collocation: [70.2145] '##number##'+'y√ºzyƒ±la'\n",
      "  Collocation: [28.6968] 's'+'##number##'\n",
      "  Collocation: [55.6583] '##number##'+'dakikada'\n",
      "  Collocation: [41.7253] '##number##'+'y√ºzyƒ±l'\n",
      "  Collocation: [46.1923] '##number##'+'y√ºzyƒ±ldan'\n",
      "  Collocation: [73.8136] 'iÃá'+'s'\n",
      "  Collocation: [84.1135] 'a'+'≈ü.'\n",
      "  Collocation: [22.3514] '√∂'+'##number##'\n",
      "  Collocation: [19.6226] '##number##'+'y√ºzyƒ±llarda'\n",
      "  Collocation: [14.2586] '##number##'+'sƒ±nƒ±f'\n",
      "  Collocation: [44.7702] 'iÃá'+'melih'\n",
      "  Collocation: [10.1821] '##number##'+'g√ºn√ºnde'\n",
      "  Collocation: [54.7198] 'b'+'iÃáplik√ßioƒülu'\n",
      "  Collocation: [17.1939] '##number##'+'satƒ±rda'\n",
      "  Collocation: [45.6196] 'i'+'al√¢eddin'\n",
      "  Collocation: [19.3874] '##number##'+'y√ºzyƒ±llar'\n",
      "  Collocation: [48.8352] 'p'+'w'\n",
      "  Collocation: [63.8468] 'w'+'ball'\n",
      "  Collocation: [30.7453] 'n'+'√ßevik'\n",
      "  Collocation: [32.7284] 't'+'kodakarensis'\n",
      "  Collocation: [34.0202] 'r'+'heberdey'\n",
      "  Collocation: [15.8984] '##number##'+'satƒ±rlar'\n",
      "  Collocation: [28.6094] 'iÃá'+'√∂'\n",
      "  Collocation: [15.8984] '##number##'+'maddesine'\n",
      "  Collocation: [15.8984] '##number##'+'maddesinde'\n",
      "  Collocation: [9.2821] '##number##'+'http'\n",
      "  Collocation: [15.8984] '##number##'+'maddesini'\n",
      "  Collocation: [15.8033] 'i'+'melih'\n",
      "  Collocation: [12.1173] '##number##'+'ayet'\n",
      "  Collocation: [16.7517] 's'+'s'\n",
      "  Collocation: [41.2351] 'f'+'t√∂'\n"
     ]
    }
   ],
   "source": [
    "trainer = PunktTrainer()\n",
    "text = do_it([i[\"text\"] for i in train_all])\n",
    "trainer.train(text, verbose=True)\n",
    "trainer.finalize_training()\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "pickle.dump(tokenizer,open(\"mukayese_punkt_clean.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.tr import Turkish\n",
    "nlp = Turkish()\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def handle_metrics(values,predictor):\n",
    "    nlp = Turkish()\n",
    "    sentences = [Doc(nlp.vocab, words=i[\"sentences\"], spaces=np.ones(len(i[\"sentences\"]), dtype=bool)) for i in values]\n",
    "    tokenized = []\n",
    "    for p in values:\n",
    "        tokens = predictor(p[\"text\"])\n",
    "        tokenized.append(Doc(nlp.vocab, words=tokens, spaces=np.ones(len(tokens), dtype=bool)))\n",
    "        \n",
    "    examples = [Example(predicted, reference) for predicted, reference in zip(tokenized, sentences)]\n",
    "    scorer = Scorer()\n",
    "    return Scorer.score_tokenization(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(sentences):\n",
    "    new = []\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        if np.random.uniform() > .5:\n",
    "            s = re.sub(r'[^\\w\\s@]', '', s)\n",
    "        if np.random.uniform() > .50:\n",
    "            u = np.random.uniform()\n",
    "            if u > .5:\n",
    "                s = s.lower()\n",
    "            else:\n",
    "                s = s.upper()\n",
    "        new.append(s)\n",
    "\n",
    "    return [i for i in new if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_abstracts = [corrupt(i[\"sentences\"]) for i in train_abstracts]\n",
    "corrupted_test_abstracts = [corrupt(i[\"sentences\"]) for i in test_abstracts]\n",
    "corrupted_train_news = [corrupt(i[\"sentences\"]) for i in train_news]\n",
    "corrupted_test_news = [corrupt(i[\"sentences\"]) for i in test_news]\n",
    "corrupted_train_tweets = [corrupt(i[\"sentences\"]) for i in train_tweets_]\n",
    "corrupted_test_tweets = [corrupt(i[\"sentences\"]) for i in final_test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_abstracts = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_train_abstracts]\n",
    "corrupted_test_abstracts = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_test_abstracts]\n",
    "corrupted_train_news = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_train_news]\n",
    "corrupted_test_news = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_test_news]\n",
    "corrupted_train_tweets = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_train_tweets]\n",
    "corrupted_test_tweets = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'iÃánternet doƒüduƒüu g√ºnden bu yana iÃángilizcenin hakim olduƒüu bir d√ºnyadƒ±r. SON YILLARDA DIƒûER DILLERINDE KATILIMIYLA SANAL D√úNYA √áOK DILLILIKLE TANI≈ûMI≈ûTIR MAKALEDE SANAL D√úNYADA DILLERIN KULLANILMA ORANINI TESPIT EDILMEYE √áALI≈ûILMI≈ûTIR √∂rneƒüin ka√ß milyon ki≈üi interneti t√ºrk√ße olarak ya da fransƒ±zca olarak kullanmaktadƒ±r. g√ºn√ºm√ºzde b√ºy√ºk bir pazar haline gelen internette t√ºketiciye ula≈ümanƒ±n √∂n√ºndeki en b√ºy√ºk engellerden birisi de dil engelidir. ƒ∞NTERNET ORTAMINDA KULLANILAN DILLERIN ORANI ETICARET VE INTERNET REKLAMCILIƒûI SEKT√ñR√úNDE √áALI≈ûANLAR I√áIN B√úY√úK √ñNEM TA≈ûIMAKTADIR ƒ∞nternet kullanƒ±cƒ±larƒ±nƒ±n dil profili i√ßin Global Reach Internet World Statsƒ±n 1996 yƒ±lƒ±ndan beri yapmƒ±≈ü olduklarƒ± √∂l√ß√ºmler esas alƒ±nmƒ±≈ütƒ±r Global Reach ise verilerini Uluslararasƒ± Telekominikasyon birliƒüinin (UIT) her √ºlkedeki kullanƒ±cƒ±lara g√∂re verdiƒüi rakamlara dayandƒ±rmaktadƒ±r. Elde edilen veriler dil atlaslarƒ± dikkate alƒ±narak rakamlara d√∂n√º≈üt√ºr√ºlmektedir Bu √ßalƒ±≈ümada internet kullanƒ±cƒ±larƒ±nƒ±n dil profili ile Arama motorlarƒ±nda indekslenen 15 milyarƒ±n √ºzerindeki web sayfasƒ±nƒ±n i√ßerikleri ile ilgili sayƒ±sal rakamlar elde edilmeye √ßalƒ±≈üƒ±lmƒ±≈ütƒ±r. BUNUN I√áIN DILBILIMSEL BAZI √ñL√á√úTLERDEN YARARLANILMI≈ûTIR Web sitelerinin dil profili ise arama motorlarƒ±ndan elde edilen rakamlardan yola √ßƒ±kƒ±larak varsayƒ±mlar √ºretilmi≈ütir her bir dilde kullanƒ±lan frekansƒ± y√ºksek kelime, hece ve ekler tespit edilerek buna g√∂re arama motorundan iÃángilizce, fransƒ±zca vb. dillerdeki sayfalar g√∂r√ºnt√ºlenmi≈ütir.', 'sentences': ['iÃánternet doƒüduƒüu g√ºnden bu yana iÃángilizcenin hakim olduƒüu bir d√ºnyadƒ±r.', 'SON YILLARDA DIƒûER DILLERINDE KATILIMIYLA SANAL D√úNYA √áOK DILLILIKLE TANI≈ûMI≈ûTIR', 'MAKALEDE SANAL D√úNYADA DILLERIN KULLANILMA ORANINI TESPIT EDILMEYE √áALI≈ûILMI≈ûTIR', '√∂rneƒüin ka√ß milyon ki≈üi interneti t√ºrk√ße olarak ya da fransƒ±zca olarak kullanmaktadƒ±r.', 'g√ºn√ºm√ºzde b√ºy√ºk bir pazar haline gelen internette t√ºketiciye ula≈ümanƒ±n √∂n√ºndeki en b√ºy√ºk engellerden birisi de dil engelidir.', 'ƒ∞NTERNET ORTAMINDA KULLANILAN DILLERIN ORANI ETICARET VE INTERNET REKLAMCILIƒûI SEKT√ñR√úNDE √áALI≈ûANLAR I√áIN B√úY√úK √ñNEM TA≈ûIMAKTADIR', 'ƒ∞nternet kullanƒ±cƒ±larƒ±nƒ±n dil profili i√ßin Global Reach Internet World Statsƒ±n 1996 yƒ±lƒ±ndan beri yapmƒ±≈ü olduklarƒ± √∂l√ß√ºmler esas alƒ±nmƒ±≈ütƒ±r', 'Global Reach ise verilerini Uluslararasƒ± Telekominikasyon birliƒüinin (UIT) her √ºlkedeki kullanƒ±cƒ±lara g√∂re verdiƒüi rakamlara dayandƒ±rmaktadƒ±r.', 'Elde edilen veriler dil atlaslarƒ± dikkate alƒ±narak rakamlara d√∂n√º≈üt√ºr√ºlmektedir Bu √ßalƒ±≈ümada internet kullanƒ±cƒ±larƒ±nƒ±n dil profili ile Arama motorlarƒ±nda indekslenen 15 milyarƒ±n √ºzerindeki web sayfasƒ±nƒ±n i√ßerikleri ile ilgili sayƒ±sal rakamlar elde edilmeye √ßalƒ±≈üƒ±lmƒ±≈ütƒ±r.', 'BUNUN I√áIN DILBILIMSEL BAZI √ñL√á√úTLERDEN YARARLANILMI≈ûTIR', 'Web sitelerinin dil profili ise arama motorlarƒ±ndan elde edilen rakamlardan yola √ßƒ±kƒ±larak varsayƒ±mlar √ºretilmi≈ütir', 'her bir dilde kullanƒ±lan frekansƒ± y√ºksek kelime, hece ve ekler tespit edilerek buna g√∂re arama motorundan iÃángilizce, fransƒ±zca vb. dillerdeki sayfalar g√∂r√ºnt√ºlenmi≈ütir.']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': 'Bu √ßalƒ±≈ümanƒ±n hedefi √º√ß farklƒ± gazetenin √ßocuk haberlerine yer veri≈ü bi√ßimlerini deƒüerlendirerek olasƒ± farklƒ±lƒ±klarƒ± belirlemek ve s√∂z konusu farklƒ±lƒ±klarƒ± √ßocuƒüun zarar g√∂rebilirliƒüi bakƒ±mƒ±ndan incelemektir Bu ama√ßla H√ºrriyet, Cumhuriyet ve Zaman gazetelerinde 1 Temmuz 2006-31 Aralƒ±k 2006 tarihleri arasƒ±nda yer alan √ßocuk haberleri incelenmi≈ütir. √ñnceden yapmƒ±≈ü olduƒüumuz pilot √ßalƒ±≈ümanƒ±n sonu√ßlarƒ± doƒürultusunda anƒ±lan √º√ß gazete se√ßilmi≈ütir. pilot √ßalƒ±≈üma aynƒ± √ßocuk haberinde en b√ºy√ºk ifade farkƒ±nƒ±n bu gazeteler arasƒ±nda olduƒüunu ortaya koymu≈ütur √ßalƒ±≈ümanƒ±n hedefinin √ßocuƒüun zarar g√∂rebilirliƒüini ortaya koymak olmasƒ± bakƒ±mƒ±ndan aynƒ± haberi zƒ±t √ºslup ve ifade ile sunan gazeteler se√ßilmi≈ütir bu gazetelerdeki √ßocuk haberleri i√ßerik analizi y√∂ntemi kullanƒ±larak deƒüerlendirilmi≈ütir. Anƒ±lan s√ºre i√ßinde √º√ß gazetede 1006 √ßocuk haberinin yayƒ±nlandƒ±ƒüƒ± tesbit edilmi≈ütir En √∂nemli bulgulara baktƒ±ƒüƒ±mƒ±zda √ßocuklarƒ±n en √ßok adli haberlerde yer aldƒ±ƒüƒ± ve √ßocuklarƒ±n ba≈üarƒ±sƒ±na ili≈ükin haberlerin yok denecek kadar az olduƒüu g√∂r√ºlmektedir Yasalarla d√ºzenlenmi≈ü olmasƒ±na raƒümen bazƒ± haberlerde √ßocuƒüun kimliƒüi ve fotoƒürafƒ± a√ßƒ±k, bir kƒ±smƒ±nda da kƒ±smen a√ßƒ±k bir bi√ßimde belirtilmi≈ütir. √áOCUKLARIN √ñNEMLI BIR B√ñL√úM√ú HABERLERDE SU√á MAƒûDURU KONUMUNDA YER ALMAKTADIR √áoƒüunluƒüu √º√ß√ºnc√º sayfada yer alan haberlerde su√ß maƒüduru ve ≈ü√ºpheli √ßocuklarƒ±n kimliklerinin sosyal √ßevreleri tarafƒ±ndan tanƒ±nmalarƒ±na yol a√ßabilecek ≈üekilde verildiƒüi g√∂r√ºlmektedir. √áocuklarƒ±n su√ß maƒüduru ya da ≈ü√ºpheli konumunda olduklarƒ± haberlerde √ßocuƒüun kendisinin ve ailesinin kimliƒüinin ya da fotoƒürafƒ±nƒ±n gerektiƒüi ≈üekilde verilmediƒüi g√∂zlenmektedir. FOTOƒûRAF KULLANIMI VE KIMLIK KONUSUNDA EN DUYARLI GAZETE CUMHURIYET GAZETESIDIR Ancak bu gazetede yer alan √ßocuk haberi sayƒ±sƒ±nƒ±n azlƒ±ƒüƒ± g√∂z ardƒ± edilmemelidir H√úRRIYET GAZETESININ BU KONUDA √áOK FAZLA √ñZEN G√ñSTERMEDIƒûI G√ñZLENMEKTEDIR. √áocuklarƒ±n b√ºy√ºk bir √ßoƒüunluƒüu i√ßin herhangi bir tanƒ±mlama kullanƒ±lmazken, ≈ü√ºpheli konumunda yer alan √ßocuklarƒ±n tamamƒ±na yakƒ±nƒ± i√ßin olumsuz tanƒ±mlamalarƒ±n kullanƒ±ldƒ±ƒüƒ± tesbit edilmi≈ütir.', 'sentences': ['Bu √ßalƒ±≈ümanƒ±n hedefi √º√ß farklƒ± gazetenin √ßocuk haberlerine yer veri≈ü bi√ßimlerini deƒüerlendirerek olasƒ± farklƒ±lƒ±klarƒ± belirlemek ve s√∂z konusu farklƒ±lƒ±klarƒ± √ßocuƒüun zarar g√∂rebilirliƒüi bakƒ±mƒ±ndan incelemektir', 'Bu ama√ßla H√ºrriyet, Cumhuriyet ve Zaman gazetelerinde 1 Temmuz 2006-31 Aralƒ±k 2006 tarihleri arasƒ±nda yer alan √ßocuk haberleri incelenmi≈ütir.', '√ñnceden yapmƒ±≈ü olduƒüumuz pilot √ßalƒ±≈ümanƒ±n sonu√ßlarƒ± doƒürultusunda anƒ±lan √º√ß gazete se√ßilmi≈ütir.', 'pilot √ßalƒ±≈üma aynƒ± √ßocuk haberinde en b√ºy√ºk ifade farkƒ±nƒ±n bu gazeteler arasƒ±nda olduƒüunu ortaya koymu≈ütur', '√ßalƒ±≈ümanƒ±n hedefinin √ßocuƒüun zarar g√∂rebilirliƒüini ortaya koymak olmasƒ± bakƒ±mƒ±ndan aynƒ± haberi zƒ±t √ºslup ve ifade ile sunan gazeteler se√ßilmi≈ütir', 'bu gazetelerdeki √ßocuk haberleri i√ßerik analizi y√∂ntemi kullanƒ±larak deƒüerlendirilmi≈ütir.', 'Anƒ±lan s√ºre i√ßinde √º√ß gazetede 1006 √ßocuk haberinin yayƒ±nlandƒ±ƒüƒ± tesbit edilmi≈ütir', 'En √∂nemli bulgulara baktƒ±ƒüƒ±mƒ±zda √ßocuklarƒ±n en √ßok adli haberlerde yer aldƒ±ƒüƒ± ve √ßocuklarƒ±n ba≈üarƒ±sƒ±na ili≈ükin haberlerin yok denecek kadar az olduƒüu g√∂r√ºlmektedir', 'Yasalarla d√ºzenlenmi≈ü olmasƒ±na raƒümen bazƒ± haberlerde √ßocuƒüun kimliƒüi ve fotoƒürafƒ± a√ßƒ±k, bir kƒ±smƒ±nda da kƒ±smen a√ßƒ±k bir bi√ßimde belirtilmi≈ütir.', '√áOCUKLARIN √ñNEMLI BIR B√ñL√úM√ú HABERLERDE SU√á MAƒûDURU KONUMUNDA YER ALMAKTADIR', '√áoƒüunluƒüu √º√ß√ºnc√º sayfada yer alan haberlerde su√ß maƒüduru ve ≈ü√ºpheli √ßocuklarƒ±n kimliklerinin sosyal √ßevreleri tarafƒ±ndan tanƒ±nmalarƒ±na yol a√ßabilecek ≈üekilde verildiƒüi g√∂r√ºlmektedir.', '√áocuklarƒ±n su√ß maƒüduru ya da ≈ü√ºpheli konumunda olduklarƒ± haberlerde √ßocuƒüun kendisinin ve ailesinin kimliƒüinin ya da fotoƒürafƒ±nƒ±n gerektiƒüi ≈üekilde verilmediƒüi g√∂zlenmektedir.', 'FOTOƒûRAF KULLANIMI VE KIMLIK KONUSUNDA EN DUYARLI GAZETE CUMHURIYET GAZETESIDIR', 'Ancak bu gazetede yer alan √ßocuk haberi sayƒ±sƒ±nƒ±n azlƒ±ƒüƒ± g√∂z ardƒ± edilmemelidir', 'H√úRRIYET GAZETESININ BU KONUDA √áOK FAZLA √ñZEN G√ñSTERMEDIƒûI G√ñZLENMEKTEDIR.', '√áocuklarƒ±n b√ºy√ºk bir √ßoƒüunluƒüu i√ßin herhangi bir tanƒ±mlama kullanƒ±lmazken, ≈ü√ºpheli konumunda yer alan √ßocuklarƒ±n tamamƒ±na yakƒ±nƒ± i√ßin olumsuz tanƒ±mlamalarƒ±n kullanƒ±ldƒ±ƒüƒ± tesbit edilmi≈ütir.']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': 'Twitter yasaƒüƒ± kalktƒ± ama bu kez b√ºy√ºk bir iddia ortaya atƒ±ldƒ±. BILGISAYAR UZMANLARI, TELEKOM\\'UN TWITTER\\'DE IP\\'LERI (BILGISAYARLARIN KIMLIK NUMARASI) TESBIT ETMEK I√áIN BIR Y√ñNTEM UYGULADIƒûI √ñNE S√úR√úL√úYOR. TWITTER\\'DAKI YAVA≈ûLAMANIN DA BURADAN KAYNAKLANDIƒûI BELIRTILIYOR. Azmedilen blog sitesinde Krototip adlƒ± yazar durumu t√ºm a√ßƒ±klƒ±ƒüƒ±yla anlatƒ±yor: \"Twitter Yasaƒüƒ± Kalktƒ± (!) Az √∂nce g√∂rd√ºƒü√ºm bir ba≈ülƒ±ktƒ± \" Twitter yasaƒüƒ± kalktƒ±!\" aklƒ±ma nedense TIB ve AKPli kurmaylarƒ±n yaptƒ±ƒüƒ± a√ßƒ±klama geldi  twitter mahkeme kararlarƒ±nƒ± uygulamazsa yasaƒüƒ± kaldƒ±rmayacaƒüƒ±z PEKI NE DEƒûI≈ûMI≈ûTI? TWITTER HALA MAHKEME KARARLARINI UYGULAMIYORDU HEMEN BILDIƒûIM BIR KA√á BILGISAYAR KOMUTUYLA OLAYI ARA≈ûTIRMAYA BA≈ûLADIM. ƒ∞lk olarak twitter sitesine bir ping atayƒ±m dedim. SONU√áLAR BIRAZ TUHAFTI Tuhaflƒ±ƒüƒ±n nedenini a√ßƒ±klayacaƒüƒ±m. sonra google a da ping attƒ±m farkƒ± anlayabildiniz mi? Anlayamadƒ±ysanƒ±z ≈ü√∂yle a√ßƒ±klayayƒ±m Veri alƒ±≈üveri≈üindeki zaman farklƒ±lƒ±klarƒ±. Tabi bu zaman farklƒ±lƒ±klarƒ±na bir √ßok etken sebep olabilir Lakin Twitter ƒ±n kullandƒ±ƒüƒ± trafik aƒüƒ± google a neredeyse aynƒ± √á√úNK√ú HIZ KAVRAMI G√úN√úM√úZ B√úY√úK INTERNET SITELERINDE √áOK √ñNEMLI BIR KAVRAMDIR Twitter ƒ±n s√ºrelerine bakarsanƒ±z eƒüer 220 ile 250 ms arasƒ±nda zamanlarda veri alƒ±≈üveri≈üi yaptƒ±ƒüƒ±nƒ± g√∂r√ºrs√ºn√ºz oysa google.com 25 ile 40 saniye arasƒ±nda bu i≈ülemi yapƒ±yor. BU S√úRELER SANIYENIN MILYONDA BIRINE DENK GELSE DE INTERNET KULLANICILARI I√áIN √áOK √ñNEMSIZ G√ñR√úNSE DE BIZLERE √áOK BILGI VERIYOR Bununla yetinmedim tor kullanan arkada≈üƒ±n bilgisayarƒ±ndan bir ping attƒ±m ARKADA≈ûIMIN BILGISAYARINDA TWITTERCOM I√áIN √áIKAN S√úRE DEƒûERLERI 45 ILE 60 SANIYE ARASINDA S√úRELERDI TABI BU A√áIKLAMA ILE YETINMEYECEƒûIM BA≈ûKA BIR KA√á I≈ûLEMDEN DAHA GE√áIRDIM TWITTER.COM U SONU√áLARI MI? hadi beraber g√∂relim √ñnce bir nslookup komutu ile siteye baktƒ±m  NSLOOKUP  DOMAIN ADLARI VE IP ADRESLERI ILE ILGILI √áE≈ûITLI SORGULAR YAPMAMIZA YARDIMCI OLAN BIR DOS KOMUTUDUR VE TCPIP ILE BERABER GELIR g√∂zlerime inanamadƒ±m. A√ßƒ±k√ßasƒ± i√ßimden \" Vayy K√∂y Kurnazlarƒ±na bak Sen \" dedim. Hala verilerimiz 195.175.254.2 duvarƒ±ndan ge√ßiyordu. BU IP ADRESI √ñNCEKI YAZILARDA BELIRTTIƒûIM GIBI TELEKOM‚ÄôUN YASAKLANAN SITELER I√áIN KULLANDIƒûI BIR IP ADRESIYDI. Sonra bir de Tracert komutuna baktƒ±m (TRACERT KOMUTU ILE G√ñNDERILEN √ú√á PAKETIN HANGI Aƒû GE√áITLERINDEN GE√áTIƒûI TAKIP EDILIR VE BU YOLLA BIR PAKETIN U√á NOKTAYA HANGI YOLLARI IZLEYEREK GITTIƒûI G√ñR√úL√úR) HADI HEP BERABER G√ñRELIM ASLINDA TWITTERA GIRERKEN BILGILERIMIZ NERELERE KAYDOLUYOR. ≈ûa≈üƒ±rmaya hazƒ±r mƒ±sƒ±nƒ±z vayy Gezintimiz kendi yerel hostlarƒ±mƒ±zdan ba≈ülƒ±yor sonra Ulus Incesu T√ºrk Telekom‚Äôa gidiyor verilerimiz. Orada √ßay ikram ediyorlar. sonra onlarƒ±n bir yedeƒüi alƒ±nƒ±yor Ordan hoop Frankurta biraz orda geziniyoruz SONRA TWITTERIN SUNUCULARINA Y√ñNLENEBILIYOR VERILERIMIZ EEE BUNUN NE ZARARI VAR DIYECEKSINIZ? Twittera yaptƒ±rƒ±ma g√ºc√º yetmeyen AKP h√ºk√ºmeti bu ≈üekilde bir DNS Hijacking olayƒ± ile IPlerimizi √∂ƒürenme fƒ±rsatƒ±nƒ± bu ≈üekilde elde etmeye √ßalƒ±≈üƒ±yor Ki≈üisel g√ºvenliƒüiniz her ≈üeyden √∂nemlidir Not Bili≈üim uzmanlarƒ±nƒ±n da uyarƒ±larƒ±nƒ± dikkate alarak siz siz olun VPNi kullanmaya devam edin', 'sentences': ['Twitter yasaƒüƒ± kalktƒ± ama bu kez b√ºy√ºk bir iddia ortaya atƒ±ldƒ±.', \"BILGISAYAR UZMANLARI, TELEKOM'UN TWITTER'DE IP'LERI (BILGISAYARLARIN KIMLIK NUMARASI) TESBIT ETMEK I√áIN BIR Y√ñNTEM UYGULADIƒûI √ñNE S√úR√úL√úYOR.\", \"TWITTER'DAKI YAVA≈ûLAMANIN DA BURADAN KAYNAKLANDIƒûI BELIRTILIYOR.\", 'Azmedilen blog sitesinde Krototip adlƒ± yazar durumu t√ºm a√ßƒ±klƒ±ƒüƒ±yla anlatƒ±yor: \"Twitter Yasaƒüƒ± Kalktƒ± (!)', 'Az √∂nce g√∂rd√ºƒü√ºm bir ba≈ülƒ±ktƒ± \" Twitter yasaƒüƒ± kalktƒ±!\"', 'aklƒ±ma nedense TIB ve AKPli kurmaylarƒ±n yaptƒ±ƒüƒ± a√ßƒ±klama geldi ', 'twitter mahkeme kararlarƒ±nƒ± uygulamazsa yasaƒüƒ± kaldƒ±rmayacaƒüƒ±z', 'PEKI NE DEƒûI≈ûMI≈ûTI?', 'TWITTER HALA MAHKEME KARARLARINI UYGULAMIYORDU', 'HEMEN BILDIƒûIM BIR KA√á BILGISAYAR KOMUTUYLA OLAYI ARA≈ûTIRMAYA BA≈ûLADIM.', 'ƒ∞lk olarak twitter sitesine bir ping atayƒ±m dedim.', 'SONU√áLAR BIRAZ TUHAFTI', 'Tuhaflƒ±ƒüƒ±n nedenini a√ßƒ±klayacaƒüƒ±m.', 'sonra google a da ping attƒ±m', 'farkƒ± anlayabildiniz mi?', 'Anlayamadƒ±ysanƒ±z ≈ü√∂yle a√ßƒ±klayayƒ±m', 'Veri alƒ±≈üveri≈üindeki zaman farklƒ±lƒ±klarƒ±.', 'Tabi bu zaman farklƒ±lƒ±klarƒ±na bir √ßok etken sebep olabilir', 'Lakin Twitter ƒ±n kullandƒ±ƒüƒ± trafik aƒüƒ± google a neredeyse aynƒ±', '√á√úNK√ú HIZ KAVRAMI G√úN√úM√úZ B√úY√úK INTERNET SITELERINDE √áOK √ñNEMLI BIR KAVRAMDIR', 'Twitter ƒ±n s√ºrelerine bakarsanƒ±z eƒüer 220 ile 250 ms arasƒ±nda zamanlarda veri alƒ±≈üveri≈üi yaptƒ±ƒüƒ±nƒ± g√∂r√ºrs√ºn√ºz oysa google.com 25 ile 40 saniye arasƒ±nda bu i≈ülemi yapƒ±yor.', 'BU S√úRELER SANIYENIN MILYONDA BIRINE DENK GELSE DE INTERNET KULLANICILARI I√áIN √áOK √ñNEMSIZ G√ñR√úNSE DE BIZLERE √áOK BILGI VERIYOR', 'Bununla yetinmedim tor kullanan arkada≈üƒ±n bilgisayarƒ±ndan bir ping attƒ±m', 'ARKADA≈ûIMIN BILGISAYARINDA TWITTERCOM I√áIN √áIKAN S√úRE DEƒûERLERI 45 ILE 60 SANIYE ARASINDA S√úRELERDI', 'TABI BU A√áIKLAMA ILE YETINMEYECEƒûIM BA≈ûKA BIR KA√á I≈ûLEMDEN DAHA GE√áIRDIM TWITTER.COM U SONU√áLARI MI?', 'hadi beraber g√∂relim √ñnce bir nslookup komutu ile siteye baktƒ±m', ' NSLOOKUP  DOMAIN ADLARI VE IP ADRESLERI ILE ILGILI √áE≈ûITLI SORGULAR YAPMAMIZA YARDIMCI OLAN BIR DOS KOMUTUDUR VE TCPIP ILE BERABER GELIR', 'g√∂zlerime inanamadƒ±m.', 'A√ßƒ±k√ßasƒ± i√ßimden \" Vayy K√∂y Kurnazlarƒ±na bak Sen \" dedim.', 'Hala verilerimiz 195.175.254.2 duvarƒ±ndan ge√ßiyordu.', 'BU IP ADRESI √ñNCEKI YAZILARDA BELIRTTIƒûIM GIBI TELEKOM‚ÄôUN YASAKLANAN SITELER I√áIN KULLANDIƒûI BIR IP ADRESIYDI.', 'Sonra bir de Tracert komutuna baktƒ±m', '( TRACERT KOMUTU ILE G√ñNDERILEN √ú√á PAKETIN HANGI Aƒû GE√áITLERINDEN GE√áTIƒûI TAKIP EDILIR VE BU YOLLA BIR PAKETIN U√á NOKTAYA HANGI YOLLARI IZLEYEREK GITTIƒûI G√ñR√úL√úR ) HADI HEP BERABER G√ñRELIM ASLINDA TWITTERA GIRERKEN BILGILERIMIZ NERELERE KAYDOLUYOR.', '≈ûa≈üƒ±rmaya hazƒ±r mƒ±sƒ±nƒ±z', 'vayy Gezintimiz kendi yerel hostlarƒ±mƒ±zdan ba≈ülƒ±yor sonra Ulus Incesu T√ºrk Telekom‚Äôa gidiyor verilerimiz.', 'Orada √ßay ikram ediyorlar.', 'sonra onlarƒ±n bir yedeƒüi alƒ±nƒ±yor', 'Ordan hoop Frankurta biraz orda geziniyoruz', 'SONRA TWITTERIN SUNUCULARINA Y√ñNLENEBILIYOR VERILERIMIZ', 'EEE BUNUN NE ZARARI VAR DIYECEKSINIZ?', 'Twittera yaptƒ±rƒ±ma g√ºc√º yetmeyen AKP h√ºk√ºmeti bu ≈üekilde bir DNS Hijacking olayƒ± ile IPlerimizi √∂ƒürenme fƒ±rsatƒ±nƒ± bu ≈üekilde elde etmeye √ßalƒ±≈üƒ±yor', 'Ki≈üisel g√ºvenliƒüiniz her ≈üeyden √∂nemlidir Not Bili≈üim uzmanlarƒ±nƒ±n da uyarƒ±larƒ±nƒ± dikkate alarak siz siz olun VPNi kullanmaya devam edin']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': 'Aƒüustos ayƒ± i√ßerisinde duyurulan ve t√ºm d√ºnyada satƒ±≈üa √ßƒ±karƒ±lan Samsung Galaxy Note 7 modelinde ≈üok etkisi yaratan sorun sebebiyle cihazlar geri toplatƒ±lmƒ±≈ütƒ± 2 EYL√úL TARIHINDE SAMSUNG K√úRESEL OLARAK CIHAZLARI GERI √áAƒûIRDIƒûINI DUYURMU≈ûTU Yakla≈üƒ±k 25 milyon Galaxy Note 7 Samsung satƒ±≈ü noktalarƒ±nda toplandƒ± ve para iadesi yapƒ±ldƒ± para iadesi almayan kullanƒ±cƒ±lar ise deƒüi≈üim cihazlarƒ± gelene kadar beklemek zorunda Peki cihaz tekrar ne zaman satƒ±≈üa √ßƒ±kacak? Samsung o tarihi a√ßƒ±kladƒ±. G√úNEY KORE MERKEZLI HABERE G√ñRE GALAXY NOTE 7 MODELI 28 EYL√úL TARIHINDE ILK ETAPTA G√úNEY KORE‚ÄôDE OLMAK √úZERE TEKRAR SATI≈ûA √áIKACAK. CIHAZ ILK OLARAK G√úNEY KORE‚ÄôDE TOPLATILMI≈ûTI. DOLAYISIYLA TEKRAR ORADA SATI≈ûINA BA≈ûLANMASI PEK S√úRPRIZ SAYILMAZ. Ancak √ºlkemizde tekrar ne zaman satƒ±≈üa √ßƒ±kacaƒüƒ± konusundaki belirsizlik s√ºr√ºyor. √á√úNK√ú SAMSUNG T√úRKIYE KONU HAKKINDA SESSIZLIƒûINI KORUYOR BEKLENTILER EKIM AYINI I≈ûARET EDIYOR Batarya h√ºcreleri saƒülam olan yeni cihazlarƒ±n Ekim ayƒ± i√ßerisinde k√ºresel olarak tekrar satƒ±≈üa √ßƒ±karƒ±lmasƒ± bekleniyor Yapƒ±lan a√ßƒ±klama g√∂n√ºll√º geri √ßaƒüƒ±rma olduƒüu i√ßin cihazlarƒ±nƒ± g√∂t√ºrmeyen kullanƒ±cƒ±lar i√ßin ek bir d√ºzenleme beklenmiyor. YANI HALEN GALAXY NOTE 7 KULLANIYORSANIZ CIHAZINIZI G√ñT√úRMEDIƒûINIZ S√úRECE KULLANMAYA DEVAM EDEBILIRSINIZ Ancak bunu kesinlikle √∂nermiyoruz. Yetkililer tarafƒ±ndan yapƒ±lan a√ßƒ±klamaya g√∂re Galaxy Note 7 kullanƒ±cƒ±larƒ±nƒ±n cihazlarƒ±nƒ± \"kapalƒ±\" ≈üekilde kullanmadan muhafaza etmesi gerekiyor.', 'sentences': ['Aƒüustos ayƒ± i√ßerisinde duyurulan ve t√ºm d√ºnyada satƒ±≈üa √ßƒ±karƒ±lan Samsung Galaxy Note 7 modelinde ≈üok etkisi yaratan sorun sebebiyle cihazlar geri toplatƒ±lmƒ±≈ütƒ±', '2 EYL√úL TARIHINDE SAMSUNG K√úRESEL OLARAK CIHAZLARI GERI √áAƒûIRDIƒûINI DUYURMU≈ûTU', 'Yakla≈üƒ±k 25 milyon Galaxy Note 7 Samsung satƒ±≈ü noktalarƒ±nda toplandƒ± ve para iadesi yapƒ±ldƒ±', 'para iadesi almayan kullanƒ±cƒ±lar ise deƒüi≈üim cihazlarƒ± gelene kadar beklemek zorunda', 'Peki cihaz tekrar ne zaman satƒ±≈üa √ßƒ±kacak?', 'Samsung o tarihi a√ßƒ±kladƒ±.', 'G√úNEY KORE MERKEZLI HABERE G√ñRE GALAXY NOTE 7 MODELI 28 EYL√úL TARIHINDE ILK ETAPTA G√úNEY KORE‚ÄôDE OLMAK √úZERE TEKRAR SATI≈ûA √áIKACAK.', 'CIHAZ ILK OLARAK G√úNEY KORE‚ÄôDE TOPLATILMI≈ûTI.', 'DOLAYISIYLA TEKRAR ORADA SATI≈ûINA BA≈ûLANMASI PEK S√úRPRIZ SAYILMAZ.', 'Ancak √ºlkemizde tekrar ne zaman satƒ±≈üa √ßƒ±kacaƒüƒ± konusundaki belirsizlik s√ºr√ºyor.', '√á√úNK√ú SAMSUNG T√úRKIYE KONU HAKKINDA SESSIZLIƒûINI KORUYOR', 'BEKLENTILER EKIM AYINI I≈ûARET EDIYOR', 'Batarya h√ºcreleri saƒülam olan yeni cihazlarƒ±n Ekim ayƒ± i√ßerisinde k√ºresel olarak tekrar satƒ±≈üa √ßƒ±karƒ±lmasƒ± bekleniyor', 'Yapƒ±lan a√ßƒ±klama g√∂n√ºll√º geri √ßaƒüƒ±rma olduƒüu i√ßin cihazlarƒ±nƒ± g√∂t√ºrmeyen kullanƒ±cƒ±lar i√ßin ek bir d√ºzenleme beklenmiyor.', 'YANI HALEN GALAXY NOTE 7 KULLANIYORSANIZ CIHAZINIZI G√ñT√úRMEDIƒûINIZ S√úRECE KULLANMAYA DEVAM EDEBILIRSINIZ', 'Ancak bunu kesinlikle √∂nermiyoruz.', 'Yetkililer tarafƒ±ndan yapƒ±lan a√ßƒ±klamaya g√∂re Galaxy Note 7 kullanƒ±cƒ±larƒ±nƒ±n cihazlarƒ±nƒ± \"kapalƒ±\" ≈üekilde kullanmadan muhafaza etmesi gerekiyor.']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': '@user Doƒüum G√ºn√ºn√ºz Kutlu Olsun Ba≈ükanƒ±m iyi ki varsiniz Allah yolunuzu a√ßƒ±k etsin. Ankara icin yaptiklariniz inanƒ±lmaz    BirlikteBasaracagiz Saygilarimla üíê some.url', 'sentences': ['@user Doƒüum G√ºn√ºn√ºz Kutlu Olsun Ba≈ükanƒ±m', 'iyi ki varsiniz', 'Allah yolunuzu a√ßƒ±k etsin.', 'Ankara icin yaptiklariniz inanƒ±lmaz    BirlikteBasaracagiz Saygilarimla', 'üíê some.url']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': '@USER BA≈ûKANIM RTE TALIMATI VERMI≈ûTIR \" TM ITIRAZLARI REDDEDIN \" DIYE. YSK AKPNIN MA≈ûASI DURUMUNDADIR ≈ûU AN AHƒ∞ME GIDER BU HIRSIZLIK', 'sentences': ['@USER BA≈ûKANIM RTE TALIMATI VERMI≈ûTIR \" TM ITIRAZLARI REDDEDIN \" DIYE.', 'YSK AKPNIN MA≈ûASI DURUMUNDADIR ≈ûU AN AHƒ∞ME GIDER BU HIRSIZLIK']} True \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [corrupted_train_abstracts,\n",
    "corrupted_test_abstracts,\n",
    "corrupted_train_news,\n",
    "corrupted_test_news,\n",
    "corrupted_train_tweets,\n",
    "corrupted_test_tweets]:\n",
    "    print(i[0], ['text', 'sentences'] == [j for j in i[0].keys()], \"\\n\" *3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_all = corrupted_train_abstracts + corrupted_train_news + corrupted_train_tweets\n",
    "corrupted_test_all = corrupted_test_abstracts + corrupted_test_news + corrupted_test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/\"\n",
    "with open(os.path.join(path, \"train_tweets_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_tweets:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_tweets_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_tweets:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "with open(os.path.join(path, \"train_abstracts_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_abstracts:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_abstracts_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_abstracts:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "with open(os.path.join(path, \"train_news_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_news:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_news_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_news:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "with open(os.path.join(path, \"train_all_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_all:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_all_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [0.3373] yuk\n",
      "  Abbreviation: [7.3343] m.s\n",
      "  Abbreviation: [0.3373] d√ºr\n",
      "  Abbreviation: [1.0118] t.a.b\n",
      "  Abbreviation: [0.3373] fak\n",
      "  Abbreviation: [5.5007] m.√∂\n",
      "  Abbreviation: [0.3373] √ºnv\n",
      "  Abbreviation: [0.3373] iÃáb\n",
      "  Abbreviation: [0.3373] mey\n",
      "  Abbreviation: [1.3491] a.-v\n",
      "  Abbreviation: [0.3373] sat\n",
      "  Abbreviation: [0.3373] kek\n",
      "  Abbreviation: [1.8336] z.r\n",
      "  Abbreviation: [1.8336] c.y\n",
      "  Abbreviation: [1.8336] n.a\n",
      "  Abbreviation: [0.4104] lev\n",
      "  Abbreviation: [0.3373] reh\n",
      "  Abbreviation: [0.9168] mv\n",
      "  Abbreviation: [1.8336] b.b\n",
      "  Abbreviation: [3.6671] r.e\n",
      "  Abbreviation: [2.3609] //t\n",
      "  Abbreviation: [1.8336] o.o\n",
      "  Abbreviation: [0.6745] abs\n",
      "  Abbreviation: [22.9341] iÃá.√∂\n",
      "  Abbreviation: [1.8336] a.b\n",
      "  Abbreviation: [0.3373] ins\n",
      "  Abbreviation: [3.0692] iÃá.s\n",
      "  Abbreviation: [0.9168] tƒ±\n",
      "  Abbreviation: [0.3373] 'no\n",
      "  Abbreviation: [1.8336] y.y\n",
      "  Abbreviation: [0.3722] iÃá.s.4\n",
      "  Abbreviation: [0.3722] a.h.iÃá\n",
      "  Abbreviation: [1.8336] hz\n",
      "  Abbreviation: [1.8336] ƒ±r\n",
      "  Abbreviation: [0.3373] 'ti\n",
      "  Abbreviation: [1.8336] √∂.y\n",
      "  Abbreviation: [0.9168] 'z\n",
      "  Abbreviation: [0.6745] iÃá.b\n",
      "  Abbreviation: [0.3373] blm\n",
      "  Abbreviation: [1.0118] m.a.b\n",
      "  Abbreviation: [0.9168] uz\n",
      "  Abbreviation: [0.9168] mk\n",
      "  Abbreviation: [3.6671] s.k\n",
      "  Abbreviation: [0.4963] 'tur\n",
      "  Abbreviation: [0.4963] 'tir\n",
      "  Abbreviation: [0.9168] aw\n",
      "  Abbreviation: [1.8336] m.a\n",
      "  Rare Abbrev: farklidir.\n",
      "  Rare Abbrev: bieb.\n",
      "  Rare Abbrev: staterdir.\n",
      "  Rare Abbrev: ≈üeklindedir.\n",
      "  Rare Abbrev: geli≈ütirmi≈ülerdir.\n",
      "  Rare Abbrev: kaynaklanmamƒ±≈ütƒ±r.\n",
      "  Rare Abbrev: belirtildi.\n",
      "  Rare Abbrev: gidecektim.\n",
      "  Rare Abbrev: ba≈üvuruldu.\n",
      "  Rare Abbrev: planlanƒ±yor.\n",
      "  Rare Abbrev: a√ßƒ±klanmƒ±≈ütƒ±.\n",
      "  Rare Abbrev: beklenmektedir.\n",
      "  Rare Abbrev: √ßoƒüalƒ±r.\n",
      "  Rare Abbrev: baksƒ±n.\n",
      "  Rare Abbrev: birakarak.\n",
      "  Rare Abbrev: ispatladiniz.\n",
      "  Rare Abbrev: alƒ±nmaktadƒ±r.\n",
      "  Sent Starter: [567.4642] 'bu'\n",
      "  Sent Starter: [34.8684] 'mansur'\n",
      "  Sent Starter: [160.8511] 'ancak'\n",
      "  Sent Starter: [33.1297] 'her'\n",
      "  Sent Starter: [31.5753] 'someurl'\n",
      "  Sent Starter: [74.6623] 'biz'\n",
      "  Sent Starter: [39.6084] 'ben'\n",
      "  Sent Starter: [59.4712] '≈üimdi'\n",
      "  Sent Starter: [73.4246] 'ayrƒ±ca'\n",
      "  Sent Starter: [45.6276] '√ß√ºnk√º'\n",
      "  Sent Starter: [35.7020] 'bunun'\n",
      "  Sent Starter: [36.5159] '√∂zellikle'\n",
      "  Sent Starter: [43.7293] 'buna'\n",
      "  Sent Starter: [35.0146] 'iÃálk'\n",
      "  Sent Starter: [39.8072] 'b√∂ylece'\n",
      "  Sent Starter: [39.6401] 'bununla'\n",
      "  Sent Starter: [32.8833] 'iÃákinci'\n",
      "  Collocation: [380.6117] '##number##'+'yy'\n",
      "  Collocation: [24.1358] '##number##'+'y√ºzyƒ±lƒ±n'\n",
      "  Collocation: [75.6774] 'a'+'≈ü'\n",
      "  Collocation: [21.2402] '##number##'+'y√ºzyƒ±la'\n",
      "  Collocation: [19.6953] '##number##'+'y√ºzyƒ±l'\n",
      "  Collocation: [30.8311] '##number##'+'y√ºzyilda'\n",
      "  Collocation: [65.6572] 'm'+'calpurnius'\n",
      "  Collocation: [22.1258] '##number##'+'y√ºzyƒ±lda'\n",
      "  Collocation: [23.7427] '##number##'+'y√ºzyƒ±ldan'\n",
      "  Collocation: [45.3152] 'iÃá'+'s'\n",
      "  Collocation: [14.0753] '√∂'+'##number##'\n",
      "  Collocation: [9.8330] '##number##'+'dakikada'\n",
      "  Collocation: [31.7398] 'm'+'boyutlarƒ±nda'\n",
      "  Collocation: [15.6493] '##number##'+'satƒ±rlar'\n",
      "  Collocation: [10.1849] '##number##'+'satƒ±rda'\n",
      "  Collocation: [27.9214] 'm'+'kalƒ±nlƒ±ƒüƒ±ndaki'\n",
      "  Collocation: [27.5416] 'i'+'al√¢eddin'\n",
      "  Collocation: [21.7374] 'm'+'√ßapƒ±nda'\n",
      "  Collocation: [31.4145] 'g'+'heteropoda'\n",
      "  Collocation: [35.2332] 'g'+'antari'\n",
      "  Collocation: [35.2332] 'g'+'muralis'\n",
      "  Collocation: [35.2332] 'g'+'tubulosa'\n",
      "  Collocation: [35.2332] 'g'+'confertifolia'\n",
      "  Collocation: [35.2332] 'g'+'pilosa'\n",
      "  Collocation: [11.8706] '##number##'+'y√ºzyillarda'\n",
      "  Collocation: [38.1941] 'w'+'ball'\n",
      "  Collocation: [11.8706] '##number##'+'y√ºzyƒ±llar'\n",
      "  Collocation: [31.7398] 'm'+'√∂l√ß√ºlerinde'\n",
      "  Collocation: [15.6493] '##number##'+'maddesine'\n",
      "  Collocation: [9.0403] '##number##'+'http'\n",
      "  Collocation: [19.4339] 'iÃá'+'melih'\n",
      "  Collocation: [15.7848] 'i'+'melih'\n",
      "  Collocation: [19.4284] 'c'+'ba≈ükani'\n",
      "  Collocation: [16.8151] 's'+'s'\n",
      "  Collocation: [41.0025] 'f'+'t√∂'\n"
     ]
    }
   ],
   "source": [
    "trainer = PunktTrainer()\n",
    "text = do_it([i[\"text\"] for i in corrupted_train_all])\n",
    "trainer.train(text, verbose=True)\n",
    "trainer.finalize_training()\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "pickle.dump(tokenizer,open(\"mukayese_punkt_corrupted.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7408662712455765,\n",
       " 'token_p': 0.5096983611748872,\n",
       " 'token_r': 0.31885493536724285,\n",
       " 'token_f': 0.39229784900371706}"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7234152119058653,\n",
       " 'token_p': 0.48947156111335216,\n",
       " 'token_r': 0.3004804120647813,\n",
       " 'token_f': 0.3723685018105935}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, dummy_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_punkt = pickle.load(open(\"mukayese_punkt_clean.pkl\", \"rb\"))\n",
    "corrupted_punkt = pickle.load(open(\"mukayese_punkt_corrupted.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9733960939328531,\n",
       " 'token_p': 0.8885398723274901,\n",
       " 'token_r': 0.8639408866995074,\n",
       " 'token_f': 0.8760677356511314}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, clean_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.739462063428342,\n",
       " 'token_p': 0.5210987261146497,\n",
       " 'token_r': 0.3241543261849339,\n",
       " 'token_f': 0.3996824524442002}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, clean_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9726473629782834,\n",
       " 'token_p': 0.8620464039458453,\n",
       " 'token_r': 0.843743842364532,\n",
       " 'token_f': 0.8527969329582513}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, corrupted_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7408662712455765,\n",
       " 'token_p': 0.5096983611748872,\n",
       " 'token_r': 0.31885493536724285,\n",
       " 'token_f': 0.39229784900371706}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, corrupted_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7234152119058653,\n",
       " 'token_p': 0.48947156111335216,\n",
       " 'token_r': 0.3004804120647813,\n",
       " 'token_f': 0.3723685018105935}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, dummy_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9477446994296941,\n",
       " 'token_p': 0.7646387635496085,\n",
       " 'token_r': 0.726256157635468,\n",
       " 'token_f': 0.7449533867259543}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, dummy_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\", \"w\") as data:\n",
    "    for i in train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"ƒ∞nternet doƒüduƒüu g√ºnden bu yana ƒ∞ngilizcenin hakim olduƒüu bir d√ºnyadƒ±r. Son yƒ±llarda diƒüer dillerinde katƒ±lƒ±mƒ±yla sanal d√ºnya √ßok dillilikle tanƒ±≈ümƒ±≈ütƒ±r. Makalede sanal d√ºnyada dillerin kullanƒ±lma oranƒ±nƒ± tespit edilmeye √ßalƒ±≈üƒ±lmƒ±≈ütƒ±r. √ñrneƒüin ka√ß milyon ki≈üi interneti T√ºrk√ße olarak ya da Fransƒ±zca olarak kullanmaktadƒ±r. G√ºn√ºm√ºzde b√ºy√ºk bir Pazar haline gelen internette t√ºketiciye ula≈ümanƒ±n √∂n√ºndeki en b√ºy√ºk engellerden birisi de dil engelidir. ƒ∞nternet ortamƒ±nda kullanƒ±lan dillerin oranƒ± e-ticaret ve internet reklamcƒ±lƒ±ƒüƒ± sekt√∂r√ºnde √ßalƒ±≈üanlar i√ßin b√ºy√ºk √∂nem ta≈üƒ±maktadƒ±r. ƒ∞nternet kullanƒ±cƒ±larƒ±nƒ±n dil profili i√ßin Global Reach, Internet World Stats'ƒ±n 1996 yƒ±lƒ±ndan beri yapmƒ±≈ü olduklarƒ± √∂l√ß√ºmler esas alƒ±nmƒ±≈ütƒ±r. Global Reach ise verilerini Uluslararasƒ± Telekominikasyon birliƒüinin (UIT) her √ºlkedeki kullanƒ±cƒ±lara g√∂re verdiƒüi rakamlara dayandƒ±rmaktadƒ±r. Elde edilen veriler dil atlaslarƒ± dikkate alƒ±narak rakamlara d√∂n√º≈üt√ºr√ºlmektedir Bu √ßalƒ±≈ümada internet kullanƒ±cƒ±larƒ±nƒ±n dil profili ile Arama motorlarƒ±nda indekslenen 15 milyarƒ±n √ºzerindeki web sayfasƒ±nƒ±n i√ßerikleri ile ilgili sayƒ±sal rakamlar elde edilmeye √ßalƒ±≈üƒ±lmƒ±≈ütƒ±r. Bunun i√ßin dilbilimsel bazƒ± √∂l√ß√ºtlerden yararlanƒ±lmƒ±≈ütƒ±r. Web sitelerinin dil profili ise arama motorlarƒ±ndan elde edilen rakamlardan yola √ßƒ±kƒ±larak varsayƒ±mlar √ºretilmi≈ütir. Her bir dilde kullanƒ±lan frekansƒ± y√ºksek kelime, hece ve ekler tespit edilerek buna g√∂re arama motorundan ƒ∞ngilizce, Fransƒ±zca vb. dillerdeki sayfalar g√∂r√ºnt√ºlenmi≈ütir.\",\n",
       " 'sentences': ['ƒ∞nternet doƒüduƒüu g√ºnden bu yana ƒ∞ngilizcenin hakim olduƒüu bir d√ºnyadƒ±r.',\n",
       "  'Son yƒ±llarda diƒüer dillerinde katƒ±lƒ±mƒ±yla sanal d√ºnya √ßok dillilikle tanƒ±≈ümƒ±≈ütƒ±r.',\n",
       "  'Makalede sanal d√ºnyada dillerin kullanƒ±lma oranƒ±nƒ± tespit edilmeye √ßalƒ±≈üƒ±lmƒ±≈ütƒ±r.',\n",
       "  '√ñrneƒüin ka√ß milyon ki≈üi interneti T√ºrk√ße olarak ya da Fransƒ±zca olarak kullanmaktadƒ±r.',\n",
       "  'G√ºn√ºm√ºzde b√ºy√ºk bir Pazar haline gelen internette t√ºketiciye ula≈ümanƒ±n √∂n√ºndeki en b√ºy√ºk engellerden birisi de dil engelidir.',\n",
       "  'ƒ∞nternet ortamƒ±nda kullanƒ±lan dillerin oranƒ± e-ticaret ve internet reklamcƒ±lƒ±ƒüƒ± sekt√∂r√ºnde √ßalƒ±≈üanlar i√ßin b√ºy√ºk √∂nem ta≈üƒ±maktadƒ±r.',\n",
       "  \"ƒ∞nternet kullanƒ±cƒ±larƒ±nƒ±n dil profili i√ßin Global Reach, Internet World Stats'ƒ±n 1996 yƒ±lƒ±ndan beri yapmƒ±≈ü olduklarƒ± √∂l√ß√ºmler esas alƒ±nmƒ±≈ütƒ±r.\",\n",
       "  'Global Reach ise verilerini Uluslararasƒ± Telekominikasyon birliƒüinin (UIT) her √ºlkedeki kullanƒ±cƒ±lara g√∂re verdiƒüi rakamlara dayandƒ±rmaktadƒ±r.',\n",
       "  'Elde edilen veriler dil atlaslarƒ± dikkate alƒ±narak rakamlara d√∂n√º≈üt√ºr√ºlmektedir Bu √ßalƒ±≈ümada internet kullanƒ±cƒ±larƒ±nƒ±n dil profili ile Arama motorlarƒ±nda indekslenen 15 milyarƒ±n √ºzerindeki web sayfasƒ±nƒ±n i√ßerikleri ile ilgili sayƒ±sal rakamlar elde edilmeye √ßalƒ±≈üƒ±lmƒ±≈ütƒ±r.',\n",
       "  'Bunun i√ßin dilbilimsel bazƒ± √∂l√ß√ºtlerden yararlanƒ±lmƒ±≈ütƒ±r.',\n",
       "  'Web sitelerinin dil profili ise arama motorlarƒ±ndan elde edilen rakamlardan yola √ßƒ±kƒ±larak varsayƒ±mlar √ºretilmi≈ütir.',\n",
       "  'Her bir dilde kullanƒ±lan frekansƒ± y√ºksek kelime, hece ve ekler tespit edilerek buna g√∂re arama motorundan ƒ∞ngilizce, Fransƒ±zca vb. dillerdeki sayfalar g√∂r√ºnt√ºlenmi≈ütir.']}"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\", \"w\") as data:\n",
    "    for i in test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_train.txt\", \"w\") as data:\n",
    "    for i in corrupted_train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")\n",
    "            \n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_test.txt\", \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20702"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20300"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237636"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split(\" \")) for i in open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\").read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221305"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split(\" \")) for i in open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\").read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408 61247\n",
      "3068 51712\n",
      "3408 54884\n",
      "3411 48503\n",
      "13886 121505\n",
      "13821 121090\n"
     ]
    }
   ],
   "source": [
    "for dataset in [train_abstracts,\n",
    "test_abstracts,\n",
    "train_news,\n",
    "test_news,\n",
    "train_tweets_,\n",
    "final_test_tweets,]:\n",
    "    print(sum([len(i[\"sentences\"]) for i in dataset]), sum([ sum([len(j.split(\" \")) for j in i[\"sentences\"]]) for i in dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/scratch/users/user/mukayese/ersatz_training/test_corrupted_plain.txt\",\"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        data.write(i[\"text\"] + \"\\n\")\n",
    "\n",
    "    \n",
    "with open (\"/scratch/users/user/mukayese/ersatz_training/test_plain.txt\",\"w\") as data:\n",
    "    for i in test_all:\n",
    "        data.write(i[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = open(\"/scratch/users/user/mukayese/ersatz_training/outs/corrupted_clean_500.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = Doc(nlp.vocab, words=q, spaces=np.ones(len(q), dtype=bool))\n",
    "\"\"\"y = []\n",
    "for line in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\"):\n",
    "    for j in json.loads(line)[\"sentences\"]:\n",
    "        y.append(j)\"\"\"\n",
    "references = predicted #Doc(nlp.vocab, words=y, spaces=np.ones(len(y), dtype=bool))\n",
    "scorer = Scorer()\n",
    "Scorer.score_tokenization([Example(references,predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = Doc(nlp.vocab, words=q, spaces=np.ones(len(q), dtype=bool))\n",
    "y = []\n",
    "for line in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\"):\n",
    "    for j in json.loads(line)[\"sentences\"]:\n",
    "        y.append(j)\n",
    "references = Doc(nlp.vocab, words=y, spaces=np.ones(len(y), dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\", \"w\") as data:\n",
    "    for i in test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/train_all_corrupted.jsonl\")]\n",
    "corrupted_test_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/test_all_corrupted.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\")]\n",
    "test_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_all_clean.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\", \"w\") as data:\n",
    "    for i in train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n",
    "\n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\", \"w\") as data:\n",
    "    for i in test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n",
    "\n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_train.txt\", \"w\") as data:\n",
    "    for i in corrupted_train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n",
    "\n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_test.txt\", \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.906037735849057"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i[\"sentences\"]) for i in train_all   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.830188679245283"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i[\"sentences\"]) for i in test_all   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.86811320754717"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i[\"sentences\"]) for i in test_all  + train_all ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsegmented_clean_test.txt\" , \"w\") as data:\n",
    "    for i in test_all:\n",
    "        data.write(i[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsegmented_corrupted_test.txt\" , \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        data.write(i[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/test_all_corrupted.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/mukayese/ersatz_training/test/\"\n",
    "for i, example in enumerate(corrupted_test_all):\n",
    "    with open(os.path.join(path, str(i) +  \".txt\"), \"w\") as data:\n",
    "        data.write(example[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5300"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"/scratch/users/user/mukayese/ersatz_training/test/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_4(x):\n",
    "    pad = 4 - len(x)\n",
    "    return \"0\" * pad + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0243'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_4(\"243\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1170'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder.split(\"/\")[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/mukayese/ersatz_training/test/\"\n",
    "for j,folder in enumerate(glob.glob(\"/scratch/users/user/mukayese/ersatz_training/test/*\")):\n",
    "    a = folder.split(\"/\")[-1][:-4]\n",
    "    q = os.path.join(path, pad_4(a) + \".txt\")\n",
    "    os.rename(folder,  q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = [i[\"sentences\"] for i in corrupted_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_250/\"\n",
    "\n",
    "({'token_acc': 0.7684405764017691,\n",
    "  'token_p': 0.5152147049737411,\n",
    "  'token_r': 0.3351419241396634,\n",
    "  'token_f': 0.40611207500076096},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_500/\"\n",
    "({'token_acc': 0.7681262777540055,\n",
    "  'token_p': 0.5117715167888846,\n",
    "  'token_r': 0.33308214016578747,\n",
    "  'token_f': 0.4035301278149726},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_1000/\"\n",
    "\n",
    "({'token_acc': 0.7667143538388174,\n",
    "  'token_p': 0.5107880287680767,\n",
    "  'token_r': 0.3318261743280583,\n",
    "  'token_f': 0.40230235107808504},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_250/\"\n",
    "\n",
    "({'token_acc': 0.7588465298142718,\n",
    "  'token_p': 0.5088603607151295,\n",
    "  'token_r': 0.32449399829240116,\n",
    "  'token_f': 0.396283120706575},\n",
    " 24)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_500/\"\n",
    "({'token_acc': 0.7674673008323425,\n",
    "  'token_p': 0.5047464690900672,\n",
    "  'token_r': 0.3285606631499623,\n",
    "  'token_f': 0.39802811758261825},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_1000/\"\n",
    "({'token_acc': 0.765642884395269,\n",
    "  'token_p': 0.5028204930067228,\n",
    "  'token_r': 0.32690278824415975,\n",
    "  'token_f': 0.39621262863057904},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_250/\"\n",
    "({'token_acc': 0.7642960812772134,\n",
    "  'token_p': 0.5243911988098034,\n",
    "  'token_r': 0.33639742816958007,\n",
    "  'token_f': 0.40986566296398297},\n",
    " 25)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_500/\"\n",
    "({'token_acc': 0.762773545538357,\n",
    "  'token_p': 0.520903600282375,\n",
    "  'token_r': 0.33363476513438833,\n",
    "  'token_f': 0.40674955595026646},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_1000/\"\n",
    "({'token_acc': 0.7612426467013467,\n",
    "  'token_p': 0.517425431711146,\n",
    "  'token_r': 0.33112316656620455,\n",
    "  'token_f': 0.4038225925018378},\n",
    " 25)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_250/\"\n",
    "({'token_acc': 0.7372365339578455,\n",
    "  'token_p': 0.49835146719419715,\n",
    "  'token_r': 0.3036207502636469,\n",
    "  'token_f': 0.37734435949446093},\n",
    " 23)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_500/\"\n",
    "({'token_acc': 0.7370276294645173,\n",
    "  'token_p': 0.4856345427680184,\n",
    "  'token_r': 0.2972117558402411,\n",
    "  'token_f': 0.36874746782185935},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_1000/\"\n",
    "({'token_acc': 0.7357313650067574,\n",
    "  'token_p': 0.48178603733245623,\n",
    "  'token_r': 0.2943185814035264,\n",
    "  'token_f': 0.3654110016215542},\n",
    " 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5300it [00:13, 400.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'token_acc': 0.7357313650067574,\n",
       "  'token_p': 0.48178603733245623,\n",
       "  'token_r': 0.2943185814035264,\n",
       "  'token_f': 0.3654110016215542},\n",
       " 25)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_1000/\"\n",
    "examples = []\n",
    "\n",
    "counter = 0\n",
    "for j,file in tqdm(enumerate(zip(sorted(glob.glob(path + \"*.txt\")), test_sentences))):\n",
    "    truth_file = file[1]\n",
    "    prediction_file = open(file[0] + \".out\").read().splitlines()\n",
    "    pred = Doc(nlp.vocab, words=prediction_file, spaces=np.ones(len(prediction_file), dtype=bool))\n",
    "    truth = Doc(nlp.vocab, words=truth_file, spaces=np.ones(len(truth_file), dtype=bool))\n",
    "    e = Example(pred, truth)\n",
    "    try:\n",
    "        Scorer.score_tokenization([e])\n",
    "        examples.append(e)\n",
    "    except:\n",
    "        counter += 1\n",
    "\n",
    "Scorer.score_tokenization(examples), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E949] Unable to align tokens for the predicted and reference docs. It is only possible to align the docs when both texts are the same except for whitespace and capitalization. The predicted tokens start with: [\"side m√ºzesi'nde mermerden yapƒ±lmƒ±≈ü sigma (at nalƒ±) formlu kƒ±rƒ±k par√ßalarƒ±n bir araya getirilmesiyle b√ºy√ºk b√∂l√ºm√º tamamlanan bir tabla sergilenmektedir.\", 'Alt b√∂l√ºm√º kƒ±rƒ±k tablanƒ±n dƒ±≈ü kenarlarƒ± formuna uygun fig√ºrl√º ve bitkisel bezemeli bir √ßer√ßeveye sahiptir bu √ßer√ßeve i√ßinde kazƒ±ma tekniƒüinde deniz canlƒ±larƒ±na ve bitkilere yer verildiƒüi g√∂r√ºl√ºr.', 'Tablanƒ±n ortasƒ±nda yine kazƒ±ma tekniƒüinde yapƒ±lmƒ±≈ü bir ha√ß ha√ßƒ±n saƒüƒ±nda ise sadece √ºst kƒ±smƒ± g√∂r√ºlebilen √ßift kulplu bir kap bulunmaktadƒ±r Ortada yaba ile iki yanƒ±nda ona y√∂nelmi≈ü yunus balƒ±klarƒ±ndan olu≈üan antitetik d√ºzenleme tablanƒ±n ortasƒ±ndaki ha√ßƒ±n kollarƒ±na denk gelen b√∂l√ºmlerde tekrarlanmƒ±≈ütƒ±r bu d√ºzenleme dƒ±≈üƒ±ndaki alana farklƒ± deniz hayvanlarƒ± ve bitkileri yerle≈ütirilmi≈ütir.', \"Tablanƒ±n ortasƒ±ndaki Yunanca yazƒ±t'Kuyumcularƒ±n Ortaklƒ±ƒüƒ±, Yolda≈ülƒ±ƒüƒ± ya da Dostluƒüu'gibi anlamlarƒ± i√ßermektedir.\", \"deniz ve i√ßinde ya≈üayanlara yer veren konu ve sahnelerin sorunsuz ne≈üeli keyifli huzurlu bir hayatƒ± ifade ettikleri kabul edilir ƒ∞√ñ 4 y√ºzyƒ±ldan ba≈ülayarak ≈üans sembol√º olarak kullanƒ±lan deniz ve ya≈üayanlarƒ±na ait sahneler Roma Sanatƒ±nda lahitlerde √∂zellikle resm√Æ ve √∂zel hamamlarƒ±n taban d√∂≈üemelerinde yer alƒ±r aynƒ± konulu geleneksel ikonografi, ge√ß antik-bizans d√∂nemi'nde bazƒ± taban mozaikleri, el sanatlarƒ±, g√ºm√º≈ü eserler, amforalar ve hƒ±ristiyan lahitleri √ºzerinde de devam ettirilmi≈ütir.\", 'Bu sahneler; mutluluƒüu, ≈üansƒ±, keyifli bir ya≈üamƒ±; rahat, huzurlu bir ya≈üamƒ±n vaat edildiƒüi, hayal edilen cenneti sembolize ettikleri i√ßin Hƒ±ristiyanlar tarafƒ±ndan da kabul edilmi≈ü g√∂r√ºnmektedir.', 'Bu t√ºr sahnelere yer veren ge√ß √∂rneklerden biri de masa tablalarƒ±dƒ±r.', 'konu se√ßimi a√ßƒ±sƒ±ndan side m√ºzesindeki eser denizde bulunan hayvan ve bitkileri konu olarak se√ßmi≈ü diƒüer tablalarda ise ger√ßekte olmayan canlƒ±lar ile tanrƒ± ve tanrƒ±√ßalarƒ±n ya≈üadƒ±ƒüƒ± hayali bir deniz d√ºnyasƒ± olu≈üturulmu≈ütur tablanƒ±n √ßer√ßevesindeki belli aralarla tekrarlanan, ortada yaba iki yanƒ±nda ona y√∂nelmi≈ü yunus balƒ±klarƒ±ndan olu≈üan antitetik d√ºzenlemenin ikonografisi ilgin√ßtir.', 'HIRISTIYAN INANCINDA YUNUS, BALIKLARIN KRALIDIR VE DENIZDE YA≈ûAYANLARI SEMBOLIZE EDER.', \"Yunusun koruyucu sƒ±fatƒ± ile ƒ∞sayƒ± simgelediƒüi kabul edilir hƒ±ristiyan tasvir sanatƒ±'nda yunus balƒ±ƒüƒ±na yer verili≈üinin nedenini, antik d√ºnyada olduƒüu gibi yunus balƒ±ƒüƒ±nƒ±n denizi, sevgiyi, g√ºveni, ≈üansƒ± sembolize etmesi ya da diƒüer d√ºnyaya yolculukta ruhlarƒ±n refakat√ßisi olmasƒ± ≈üeklinde a√ßƒ±klayanlar olduƒüu gibi, bunun mitolojik bir form olarak sadece dekoratif ama√ßla yapƒ±ldƒ±ƒüƒ±nƒ± kabul edenlerde vardƒ±r.\"]. The reference tokens start with: [\"AMA√á: √áALI≈ûMANIN AMACI, THERMOCOCCUS KODAKARAENSIS'DEN BIR NADH OKSIDAZIN KLONLANMASI VE ESCHERICHIA COLI DE EKSPRESYONU SONRASI GEN √úR√úN√úN√úN SAFLA≈ûTIRILMASI VE √úR√úN ENZIMIN KARAKTERIZE EDILMESIDIR.\", 'Metotlar: NADH oksidaz geni klonlamƒ±≈ü ve E. coli ekspresyon sisteminde eksprese edilmi≈ütir.', 'REKOMBINANT PROTEIN ISI MUAMELESI VE IYONDEƒûI≈ûTIRICI KOLON KROMATOGRAFISI VASITASIYLA SAFLA≈ûTIRILMI≈û VE KARAKTERIZE EDILMI≈ûTIR', 'BULGULAR: NIKOTINAMIT ADENINE DIN√úKLEOTIT OKSIDAZ HOMOLOGLARI, HIPERTERMOFILIK ARKEA GENOMLARI I√áINDE BULUNMAKTADIR.', 'Bu genomlar TK0304 TK 1299 ve TK 1392 ≈üeklinde tasarlanmƒ±≈ü √º√ß sekans halinde Thermococcus kodakaraensis KOD 1 i√ßinde bulunmaktadƒ±rlar', 'Biz bu √ßalƒ±≈ümada, TK 1392 genini ve √ºr√ºn√ºn√º karakterize etmi≈ü bulunmaktayƒ±z.', 'TK 1392 GENI 1239 N√úKLEOTITDEN MEYDANA GELMEKTE VE BU SEKANS, 413 AMINO ASIT ZINCIRINDEN OLU≈ûAN, MOLEK√úL AƒûIRLIƒûI 45, 244 DA OLAN BIR POLIPEPTIT ZINCIRINI OLU≈ûTURMAKTADIR.', 'Proteinin izoelektrik noktasƒ± 8.73 bulunmu≈ütur.', 'TK 1392 proteininin amino asit sekansƒ±, diƒüer mikroorganizmalarla ve olu≈üturulmu≈ü bir filogenetik aƒüa√ß i√ßindeki homologlarƒ±yla kar≈üƒ±la≈ütƒ±rƒ±lmƒ±≈ütƒ±r.', 'NADH oksidazƒ±n molek√ºler √∂zelliklerinin ara≈ütƒ±rƒ±lmasƒ± i√ßin TK 1392 geni klonlanmƒ±≈ü ve gen √ºr√ºn√º eksprese edilmi≈ütir'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-58b8604966a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruth_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mScorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/scorer.py\u001b[0m in \u001b[0;36mscore_tokenization\u001b[0;34m(examples, **cfg)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgold_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_unknown_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0malign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mgold_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpred_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.Example.alignment.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/training/alignment.py\u001b[0m in \u001b[0;36mfrom_strings\u001b[0;34m(cls, A, B)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Alignment\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx2y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_alignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAlignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx2y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my2x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/training/align.pyx\u001b[0m in \u001b[0;36mspacy.training.align.get_alignments\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E949] Unable to align tokens for the predicted and reference docs. It is only possible to align the docs when both texts are the same except for whitespace and capitalization. The predicted tokens start with: [\"side m√ºzesi'nde mermerden yapƒ±lmƒ±≈ü sigma (at nalƒ±) formlu kƒ±rƒ±k par√ßalarƒ±n bir araya getirilmesiyle b√ºy√ºk b√∂l√ºm√º tamamlanan bir tabla sergilenmektedir.\", 'Alt b√∂l√ºm√º kƒ±rƒ±k tablanƒ±n dƒ±≈ü kenarlarƒ± formuna uygun fig√ºrl√º ve bitkisel bezemeli bir √ßer√ßeveye sahiptir bu √ßer√ßeve i√ßinde kazƒ±ma tekniƒüinde deniz canlƒ±larƒ±na ve bitkilere yer verildiƒüi g√∂r√ºl√ºr.', 'Tablanƒ±n ortasƒ±nda yine kazƒ±ma tekniƒüinde yapƒ±lmƒ±≈ü bir ha√ß ha√ßƒ±n saƒüƒ±nda ise sadece √ºst kƒ±smƒ± g√∂r√ºlebilen √ßift kulplu bir kap bulunmaktadƒ±r Ortada yaba ile iki yanƒ±nda ona y√∂nelmi≈ü yunus balƒ±klarƒ±ndan olu≈üan antitetik d√ºzenleme tablanƒ±n ortasƒ±ndaki ha√ßƒ±n kollarƒ±na denk gelen b√∂l√ºmlerde tekrarlanmƒ±≈ütƒ±r bu d√ºzenleme dƒ±≈üƒ±ndaki alana farklƒ± deniz hayvanlarƒ± ve bitkileri yerle≈ütirilmi≈ütir.', \"Tablanƒ±n ortasƒ±ndaki Yunanca yazƒ±t'Kuyumcularƒ±n Ortaklƒ±ƒüƒ±, Yolda≈ülƒ±ƒüƒ± ya da Dostluƒüu'gibi anlamlarƒ± i√ßermektedir.\", \"deniz ve i√ßinde ya≈üayanlara yer veren konu ve sahnelerin sorunsuz ne≈üeli keyifli huzurlu bir hayatƒ± ifade ettikleri kabul edilir ƒ∞√ñ 4 y√ºzyƒ±ldan ba≈ülayarak ≈üans sembol√º olarak kullanƒ±lan deniz ve ya≈üayanlarƒ±na ait sahneler Roma Sanatƒ±nda lahitlerde √∂zellikle resm√Æ ve √∂zel hamamlarƒ±n taban d√∂≈üemelerinde yer alƒ±r aynƒ± konulu geleneksel ikonografi, ge√ß antik-bizans d√∂nemi'nde bazƒ± taban mozaikleri, el sanatlarƒ±, g√ºm√º≈ü eserler, amforalar ve hƒ±ristiyan lahitleri √ºzerinde de devam ettirilmi≈ütir.\", 'Bu sahneler; mutluluƒüu, ≈üansƒ±, keyifli bir ya≈üamƒ±; rahat, huzurlu bir ya≈üamƒ±n vaat edildiƒüi, hayal edilen cenneti sembolize ettikleri i√ßin Hƒ±ristiyanlar tarafƒ±ndan da kabul edilmi≈ü g√∂r√ºnmektedir.', 'Bu t√ºr sahnelere yer veren ge√ß √∂rneklerden biri de masa tablalarƒ±dƒ±r.', 'konu se√ßimi a√ßƒ±sƒ±ndan side m√ºzesindeki eser denizde bulunan hayvan ve bitkileri konu olarak se√ßmi≈ü diƒüer tablalarda ise ger√ßekte olmayan canlƒ±lar ile tanrƒ± ve tanrƒ±√ßalarƒ±n ya≈üadƒ±ƒüƒ± hayali bir deniz d√ºnyasƒ± olu≈üturulmu≈ütur tablanƒ±n √ßer√ßevesindeki belli aralarla tekrarlanan, ortada yaba iki yanƒ±nda ona y√∂nelmi≈ü yunus balƒ±klarƒ±ndan olu≈üan antitetik d√ºzenlemenin ikonografisi ilgin√ßtir.', 'HIRISTIYAN INANCINDA YUNUS, BALIKLARIN KRALIDIR VE DENIZDE YA≈ûAYANLARI SEMBOLIZE EDER.', \"Yunusun koruyucu sƒ±fatƒ± ile ƒ∞sayƒ± simgelediƒüi kabul edilir hƒ±ristiyan tasvir sanatƒ±'nda yunus balƒ±ƒüƒ±na yer verili≈üinin nedenini, antik d√ºnyada olduƒüu gibi yunus balƒ±ƒüƒ±nƒ±n denizi, sevgiyi, g√ºveni, ≈üansƒ± sembolize etmesi ya da diƒüer d√ºnyaya yolculukta ruhlarƒ±n refakat√ßisi olmasƒ± ≈üeklinde a√ßƒ±klayanlar olduƒüu gibi, bunun mitolojik bir form olarak sadece dekoratif ama√ßla yapƒ±ldƒ±ƒüƒ±nƒ± kabul edenlerde vardƒ±r.\"]. The reference tokens start with: [\"AMA√á: √áALI≈ûMANIN AMACI, THERMOCOCCUS KODAKARAENSIS'DEN BIR NADH OKSIDAZIN KLONLANMASI VE ESCHERICHIA COLI DE EKSPRESYONU SONRASI GEN √úR√úN√úN√úN SAFLA≈ûTIRILMASI VE √úR√úN ENZIMIN KARAKTERIZE EDILMESIDIR.\", 'Metotlar: NADH oksidaz geni klonlamƒ±≈ü ve E. coli ekspresyon sisteminde eksprese edilmi≈ütir.', 'REKOMBINANT PROTEIN ISI MUAMELESI VE IYONDEƒûI≈ûTIRICI KOLON KROMATOGRAFISI VASITASIYLA SAFLA≈ûTIRILMI≈û VE KARAKTERIZE EDILMI≈ûTIR', 'BULGULAR: NIKOTINAMIT ADENINE DIN√úKLEOTIT OKSIDAZ HOMOLOGLARI, HIPERTERMOFILIK ARKEA GENOMLARI I√áINDE BULUNMAKTADIR.', 'Bu genomlar TK0304 TK 1299 ve TK 1392 ≈üeklinde tasarlanmƒ±≈ü √º√ß sekans halinde Thermococcus kodakaraensis KOD 1 i√ßinde bulunmaktadƒ±rlar', 'Biz bu √ßalƒ±≈ümada, TK 1392 genini ve √ºr√ºn√ºn√º karakterize etmi≈ü bulunmaktayƒ±z.', 'TK 1392 GENI 1239 N√úKLEOTITDEN MEYDANA GELMEKTE VE BU SEKANS, 413 AMINO ASIT ZINCIRINDEN OLU≈ûAN, MOLEK√úL AƒûIRLIƒûI 45, 244 DA OLAN BIR POLIPEPTIT ZINCIRINI OLU≈ûTURMAKTADIR.', 'Proteinin izoelektrik noktasƒ± 8.73 bulunmu≈ütur.', 'TK 1392 proteininin amino asit sekansƒ±, diƒüer mikroorganizmalarla ve olu≈üturulmu≈ü bir filogenetik aƒüa√ß i√ßindeki homologlarƒ±yla kar≈üƒ±la≈ütƒ±rƒ±lmƒ±≈ütƒ±r.', 'NADH oksidazƒ±n molek√ºler √∂zelliklerinin ara≈ütƒ±rƒ±lmasƒ± i√ßin TK 1392 geni klonlanmƒ±≈ü ve gen √ºr√ºn√º eksprese edilmi≈ütir']."
     ]
    }
   ],
   "source": [
    "pred = Doc(nlp.vocab, words=prediction_file, spaces=np.ones(len(prediction_file), dtype=bool))\n",
    "truth = Doc(nlp.vocab, words=truth_file, spaces=np.ones(len(truth_file), dtype=bool))\n",
    "e = Example(pred, truth)\n",
    "Scorer.score_tokenization([e])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [i[\"sentences\"] for i in corrupted_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [i[\"text\"] for i in corrupted_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_4(x):\n",
    "    pad = 4 - len(x)\n",
    "    return \"0\" * pad + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5300it [03:37, 24.32it/s]\n"
     ]
    }
   ],
   "source": [
    "folders = [\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_1000/\",\n",
    "           \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_500/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_1000/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_500/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_1000/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_500/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_1000/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_500/\",]\n",
    "\n",
    "for j,i in tqdm(enumerate(test_text)):\n",
    "    for folder in folders: \n",
    "        with open(os.path.join(folder, pad_4(str(j)) + \".txt\"), \"w\") as data:\n",
    "            data.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avrupa Birliƒüi (AB) d√∂rt temel √∂zg√ºrl√ºk √ºzerine kurulmu≈ütur.',\n",
       " 'BUNLAR MALLARIN KI≈ûILERIN I≈û√áILERIN HIZMETLERIN SERMAYENIN SERBEST DOLA≈ûIMIDIR',\n",
       " 'Bir ba≈üka √∂zg√ºrl√ºk ise yerle≈üme hakkƒ±dƒ±r, emeƒüin ve hizmetlerin serbest dola≈üƒ±mƒ± i√ßin olmazsa olmaz (sine qua non) nitelik ta≈üƒ±maktadƒ±r.',\n",
       " 'AB bir ekonomik b√ºt√ºnle≈üme modelidir',\n",
       " 'SERBEST DOLA≈ûIM √ñZG√úRL√úƒû√ú OLMADAN EKONOMIK B√úT√úNLE≈ûMEDEN S√ñZ EDILEMEZ.',\n",
       " '√ºye √ºlkeler arasƒ±nda 0 (sƒ±fƒ±r) g√ºmr√ºk, 3. √ºlkelere kar≈üƒ± ortak g√ºmr√ºk tarifesi (ogt) uygulamasƒ± ve miktar kƒ±sƒ±tlamalarƒ±nƒ±n (kotalarƒ±n) kaldƒ±rƒ±lmasƒ±yla mallarƒ±n serbest dola≈üƒ±mƒ± saƒülanmƒ±≈ütƒ±r.',\n",
       " 'ANCAK I√á PAZARIN GER√áEKLE≈ûMESI I√áIN MALLAR YANINDA DIƒûER √úRETIM FAKT√ñRLERININ (SERMAYE, I≈û√áI, HIZMETLI) SERBEST DOLA≈ûIMININ DA SAƒûLANMASI GEREKIYORDU.',\n",
       " 'Avrupa Topluluƒüunu kuran Anla≈ümanƒ±n Topluluk Politikalarƒ± ba≈ülƒ±klƒ± 3 kƒ±smƒ± II',\n",
       " 'ba≈ülƒ±ƒüƒ±nda 23-31 maddeler mallarƒ±n serbest dola≈üƒ±mƒ±na, III.',\n",
       " 'BA≈ûLIK ISE KI≈ûILERIN HIZMETLERIN VE SERMAYENIN SERBEST DOLA≈ûIMINA AYRILMI≈ûTIR',\n",
       " 'Bu ba≈ülƒ±k altƒ±nda 1. b√∂l√ºmde i≈ü√ßiler (m.',\n",
       " '3942 2 B√ñL√úMDE YERLE≈ûME HAKKI M',\n",
       " '4348 3 b√∂l√ºmde hizmetler m',\n",
       " '49-55) 4. B√ñL√úMDE SERMAYE VE √ñDEMELER (M.',\n",
       " '5660 yer almƒ±≈ütƒ±r',\n",
       " 'Ayrƒ±ca ATyi kuran Anla≈ümaya Maastrich Anla≈ümasƒ±yla giren Avrupa vatanda≈ülƒ±ƒüƒ± kavramƒ±nƒ±n bir sonucu olarak Avrupa vatanda≈ülƒ±ƒüƒ± ba≈ülƒ±ƒüƒ±nƒ± ta≈üƒ±yan ikinci kƒ±sƒ±mda yer alan Nice Anla≈ümasƒ±yla deƒüi≈üik ≈üekliyle 181',\n",
       " 'maddesi eski 8 a Avrupa Birliƒüi vatanda≈ülarƒ±nƒ±n √ºye √ºlkelerde serbest√ße dola≈üƒ±m ve yerle≈üme hakkƒ±ndan s√∂z etmektedir',\n",
       " 'topluluk politikalarƒ± ba≈ülƒ±klƒ± 3. kƒ±smƒ±n ki≈üilerin serbest dola≈üƒ±mƒ± ile ilgili 4. b√∂l√ºm√º vize, sƒ±ƒüƒ±nma, g√∂√ß ve diƒüer politikalarla ilgilidir.',\n",
       " 'DAHA √ñNCE AVRUPA BIRLIƒûINI KURAN ANLA≈ûMANIN GETIRDIƒûI 3 S√úTUNDAADALET VE ƒ∞√áI≈ûLERINDE ƒ∞≈ûBIRLIƒûIBA≈ûLIƒûI ALTINDA YER ALAN BU KONU AMSTERDAM ANLA≈ûMASI ILE ATYI KURAN ANLA≈ûMANIN I√áINE ALINMI≈ûTIR',\n",
       " \"Bunun sonucu olarak AB Anla≈ümasƒ±'nƒ±n 3. s√ºtununun ba≈ülƒ±ƒüƒ±'Polis ve Adli ƒ∞≈ülerde ƒ∞≈übirliƒüi'olarak deƒüi≈ütirilmi≈ütir.\",\n",
       " \"Hizmetler AT'yi kuran Anla≈üma'nƒ±n 50. maddesinde mallarƒ±n, sermayenin, ve i≈ü√ßilerin serbest dola≈üƒ±mƒ±nƒ± d√ºzenleyen h√ºk√ºmler dƒ±≈üƒ±ndaki faaliyetler olarak belirlenmi≈ütir.\",\n",
       " 'BUNLAR SINAI TICARI FAALIYETLER EL SANATLARI VE SERBEST MESLEK FAALIYETLERIDIR',\n",
       " \"Avukatlarƒ±n AB'de serbest dola≈üƒ±mƒ±; doktorlar, mimarlar ve diƒüer hizmet sekt√∂rlerinde olduƒüu gibi bir √ºcret kar≈üƒ±lƒ±ƒüƒ± yapƒ±lmayan serbest meslek faaliyetleri i√ßinde deƒüerlendirilmektedir.\",\n",
       " 'AB kurumlarƒ± √∂nce bu serbest mesleklere mensup ki≈üilerin diplomalarƒ±nƒ±n tanƒ±nmasƒ± konusunda ikincil mevzuat √ßƒ±karmƒ±≈ü, daha sonra da her bir meslek mensubunun hizmet edimini serbest√ße diƒüer √ºye √ºlkelerde icra edebilmesi i√ßin gerekli kurallarƒ± belirlemi≈ütir.',\n",
       " 'Avukatlara ili≈ükin direktifler de bu amacƒ± saƒülamaya y√∂neliktir',\n",
       " 'ARA≈ûTIRMAMIZDA HIZMET EDINIMI SERBESTISINE ILI≈ûKIN GENEL KURALLARI VE DIPLOMALARIN KAR≈ûILIK TANINMASI KONUSUNDAKI MEVZUATI INCELEDIKTEN SONRA AVUKATLARA ILI≈ûKIN AB MEVZUATI VE √úYE √úLKELERDEKI UYGULAMALARI ELE ALACAK VE T√úRKIYENIN ABYE TAM √úYE OLMASI HAINDE T√úRK MEVZUATINDA YAPILMASI GEREKLI DEƒûI≈ûIKLIKLERE YER VERECEƒûIZ']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avrupa Birliƒüi (AB) d√∂rt temel √∂zg√ºrl√ºk √ºzerine kurulmu≈ütur.',\n",
       " 'BUNLAR MALLARIN KI≈ûILERIN I≈û√áILERIN HIZMETLERIN SERMAYENIN SERBEST DOLA≈ûIMIDIR Bir ba≈üka √∂zg√ºrl√ºk ise yerle≈üme hakkƒ±dƒ±r, emeƒüin ve hizmetlerin serbest dola≈üƒ±mƒ± i√ßin olmazsa olmaz (sine qua non) nitelik ta≈üƒ±maktadƒ±r.',\n",
       " 'AB bir ekonomik b√ºt√ºnle≈üme modelidir SERBEST DOLA≈ûIM √ñZG√úRL√úƒû√ú OLMADAN EKONOMIK B√úT√úNLE≈ûMEDEN S√ñZ EDILEMEZ.',\n",
       " '√ºye √ºlkeler arasƒ±nda 0 (sƒ±fƒ±r) g√ºmr√ºk, 3. √ºlkelere kar≈üƒ± ortak g√ºmr√ºk tarifesi (ogt) uygulamasƒ± ve miktar kƒ±sƒ±tlamalarƒ±nƒ±n (kotalarƒ±n) kaldƒ±rƒ±lmasƒ±yla mallarƒ±n serbest dola≈üƒ±mƒ± saƒülanmƒ±≈ütƒ±r.',\n",
       " 'ANCAK I√á PAZARIN GER√áEKLE≈ûMESI I√áIN MALLAR YANINDA DIƒûER √úRETIM FAKT√ñRLERININ (SERMAYE, I≈û√áI, HIZMETLI) SERBEST DOLA≈ûIMININ DA SAƒûLANMASI GEREKIYORDU.',\n",
       " 'Avrupa Topluluƒüunu kuran Anla≈ümanƒ±n Topluluk Politikalarƒ± ba≈ülƒ±klƒ± 3 kƒ±smƒ± II ba≈ülƒ±ƒüƒ±nda 23-31 maddeler mallarƒ±n serbest dola≈üƒ±mƒ±na, III.',\n",
       " 'BA≈ûLIK ISE KI≈ûILERIN HIZMETLERIN VE SERMAYENIN SERBEST DOLA≈ûIMINA AYRILMI≈ûTIR Bu ba≈ülƒ±k altƒ±nda 1.',\n",
       " 'b√∂l√ºmde i≈ü√ßiler (m. 3942 2 B√ñL√úMDE YERLE≈ûME HAKKI M 4348 3 b√∂l√ºmde hizmetler m 49-55) 4. B√ñL√úMDE SERMAYE VE √ñDEMELER (M. 5660 yer almƒ±≈ütƒ±r Ayrƒ±ca ATyi kuran Anla≈ümaya Maastrich Anla≈ümasƒ±yla giren Avrupa vatanda≈ülƒ±ƒüƒ± kavramƒ±nƒ±n bir sonucu olarak Avrupa vatanda≈ülƒ±ƒüƒ± ba≈ülƒ±ƒüƒ±nƒ± ta≈üƒ±yan ikinci kƒ±sƒ±mda yer alan Nice Anla≈ümasƒ±yla deƒüi≈üik ≈üekliyle 181 maddesi eski 8 a Avrupa Birliƒüi vatanda≈ülarƒ±nƒ±n √ºye √ºlkelerde serbest√ße dola≈üƒ±m ve yerle≈üme hakkƒ±ndan s√∂z etmektedir topluluk politikalarƒ± ba≈ülƒ±klƒ± 3. kƒ±smƒ±n ki≈üilerin serbest dola≈üƒ±mƒ± ile ilgili 4. b√∂l√ºm√º vize, sƒ±ƒüƒ±nma, g√∂√ß ve diƒüer politikalarla ilgilidir.',\n",
       " \"DAHA √ñNCE AVRUPA BIRLIƒûINI KURAN ANLA≈ûMANIN GETIRDIƒûI 3 S√úTUNDAADALET VE ƒ∞√áI≈ûLERINDE ƒ∞≈ûBIRLIƒûIBA≈ûLIƒûI ALTINDA YER ALAN BU KONU AMSTERDAM ANLA≈ûMASI ILE ATYI KURAN ANLA≈ûMANIN I√áINE ALINMI≈ûTIR Bunun sonucu olarak AB Anla≈ümasƒ±'nƒ±n 3. s√ºtununun ba≈ülƒ±ƒüƒ±'Polis ve Adli ƒ∞≈ülerde ƒ∞≈übirliƒüi'olarak deƒüi≈ütirilmi≈ütir.\",\n",
       " \"Hizmetler AT'yi kuran Anla≈üma'nƒ±n 50. maddesinde mallarƒ±n, sermayenin, ve i≈ü√ßilerin serbest dola≈üƒ±mƒ±nƒ± d√ºzenleyen h√ºk√ºmler dƒ±≈üƒ±ndaki faaliyetler olarak belirlenmi≈ütir.\",\n",
       " \"BUNLAR SINAI TICARI FAALIYETLER EL SANATLARI VE SERBEST MESLEK FAALIYETLERIDIR Avukatlarƒ±n AB'de serbest dola≈üƒ±mƒ±; doktorlar, mimarlar ve diƒüer hizmet sekt√∂rlerinde olduƒüu gibi bir √ºcret kar≈üƒ±lƒ±ƒüƒ± yapƒ±lmayan serbest meslek faaliyetleri i√ßinde deƒüerlendirilmektedir.\",\n",
       " 'AB kurumlarƒ± √∂nce bu serbest mesleklere mensup ki≈üilerin diplomalarƒ±nƒ±n tanƒ±nmasƒ± konusunda ikincil mevzuat √ßƒ±karmƒ±≈ü, daha sonra da her bir meslek mensubunun hizmet edimini serbest√ße diƒüer √ºye √ºlkelerde icra edebilmesi i√ßin gerekli kurallarƒ± belirlemi≈ütir.',\n",
       " 'Avukatlara ili≈ükin direktifler de bu amacƒ± saƒülamaya y√∂neliktir ARA≈ûTIRMAMIZDA HIZMET EDINIMI SERBESTISINE ILI≈ûKIN GENEL KURALLARI VE DIPLOMALARIN KAR≈ûILIK TANINMASI KONUSUNDAKI MEVZUATI INCELEDIKTEN SONRA AVUKATLARA ILI≈ûKIN AB MEVZUATI VE √úYE √úLKELERDEKI UYGULAMALARI ELE ALACAK VE T√úRKIYENIN ABYE TAM √úYE OLMASI HAINDE T√úRK MEVZUATINDA YAPILMASI GEREKLI DEƒûI≈ûIKLIKLERE YER VERECEƒûIZ']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos200/clean_250/1.txt.out\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "mustc_test = open(\"/userfiles/user/mukayese_machine_translation/en-tr/data/dev/txt/dev.en\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26510, 1304)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split(\" \")) for i in mustc_test]), len(mustc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4_628_089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
